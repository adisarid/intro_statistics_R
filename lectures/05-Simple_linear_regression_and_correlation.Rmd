---
title: "Simple Linear Regression and Correlation"
subtitle: ""
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}

.remark-code {
  font-size: 24px;
}

.huge { 
  font-size: 200%;
}
.tiny .remark-code {
  font-size: 50%;
}

.small .remark-code{
   font-size: 85% !important;
}

.small {
   font-size: 85% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

.small-slide {
   font-size: 70% !important;
}

.image-50 img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.right-plot {
   width: 60%;
   float: right;
   padding-left: 1%;
   bottom: 0px;
   right: 0px;
   position: absolute;
}



```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
library(countdown)
```

# Reminder from previous lecture

We talked about two sample statistics
   
   * Comparing variance using the $F$ test.
   
   * Goodness of fit for examining variable independence, i.e., using $\chi^2$ test.
   
   * Independence test (contingency table), also using $\chi^2$.

---

# Today

   * The project and some technicalities.
   
   * Some notes about learning `R`.
   
   * Assisted `R` Exercise.
   
   * Linear regression.

---

# Project

In the project you need to demonstrate your ability to conduct a statistical analysis on a new dataset, including
   
   * Importing and tidying
   
   * Transformations
   
   * Visualizations
   
   * Modeling
   
   * Communicating the results
   
The project should be submitted in and Rmd format, with code and data, by June 24th.

   * Weekly regular office hours are set Thursdays 9-10.


---

# Some hints to improving your `R` skills

   * One of the challenges in this course is learning `R`.
   
   * Most of this relies on self-learning.
   
   * The tools you can use:
   
      * The code from the lectures.
      
      * Workshop recordings: https://tau.cloud.panopto.eu/Panopto/Pages/Sessions/List.aspx?folderID=63ae0b2d-6a79-4d4d-82d5-ac8f0160961b
      
      * The R4DS book [https://r4ds.had.co.nz/](https://r4ds.had.co.nz/).
      
      * Additional video sources (see the syllabus, for example here https://www.youtube.com/channel/UCMdihazndR0f9XBoSXWqnYg).
      
      * Most importantly! google and stackoverflow.

---

# Example 

   * Try finding what functions should be used in `R`, and how to use them.

   * Take a theoretical problem and find how to solve it in `R`:
   
      * Compare means of two paired samples
   
      * Create one sided confidence interval for a standard deviation
      
      * Check independence of two variables

---

# Project like exercise in `R`

How to kickstart your project? with an example.

Use the IKEA dataset [Analyze the IKEA data set](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-11-03/readme.md).

   * Read the data
   
   * Compare two types of product categories (visualize + test)
   
   * Examine the relationship between volume and price (visualize + model)

---

# Simple Linear Regression: background

Linear regression is an important modeling technique in statistics. It was developed in the 1800s and is still very common today.

--

   * In essence, it allows us to model the relationship between two or more variables, utilizing some basic assumptions such as linearity, and normality.

--

   * These days, it is less common as a predictive modeling approach, because for predictions we have much better models.

--

   * It is still heavily used in research (e.g., academia) to describe and indicate statistically significant relationships.

--

   * Linear regression is very appealing as one of the "first models to try out" because it is very simple to understand, has a low computational price, it is easy to interpret, and yet very flexible.

---

# Simple Linear Regression: example - bird strikes (1/3)

A very troubling problem for aviation is bird strikes

   * From a monetary perspective - causing damages to planes
   
   * From a safety perspective - endangers the passengers and crew
   
   * (Obviously it's not that fun to the birds either)
   
--

What is the relationship between flight height and the number of bird strike events?

--

The data we will be exploring is adopted from tidytuesday (2019-07-23), [here](https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-07-23).

--

.tiny[
```{r introducing the bird strike dataset}
# wildlife_impacts <- readr::read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-07-23/wildlife_impacts.csv")
# write_csv(wildlife_impacts %>% count(height), "lectures/data/wildlife_impacts_small.csv")
wildlife_small <- read_csv("data/wildlife_impacts_small.csv", col_types = cols()) %>% 
  mutate(rounded_height = round(height/1000)) %>% 
   group_by(rounded_height) %>% 
   summarize(n = sum(n)) %>% 
   filter(!is.na(rounded_height))
```
]

---

# Bird strike events example (2/3)

The was categorized to intervals of 1000 feet, i.e., $0-999, 1000-1999,...,25000$.

Note that the data y-axis appears in **log-scale**.

.tiny[
```{r histogram of bird strike height, fig.dim=c(9, 4)}
wildlife_hist <- ggplot(wildlife_small, aes(x = rounded_height, y = n)) + 
   geom_col(fill = "darkorange", color = "black") + scale_y_log10() + theme_bw() + xlab("Height [k feet]")
wildlife_points <- ggplot(wildlife_small, aes(x = rounded_height, y = n)) + 
   geom_point() + scale_y_log10() + theme_bw() + 
   stat_smooth(method = "lm") + xlab("Height [k feet]")
cowplot::plot_grid(wildlife_hist, wildlife_points)
```
]

---

# Bird strike events example (3/3)

```{r bird strike chart smaller, echo = FALSE, fig.dim=c(6,3)}
cowplot::plot_grid(wildlife_hist, wildlife_points)
```

It would seem as though each additional $5k$ feet decrease the number of bird strikes by a ratio of $10$, or in other words:

$$\log(\text{Bird strikes}) \approx 3.784 - 0.168\times h$$

--

Equivalently, we can also write:

$$\text{Bird strikes} \approx 10^{3.784-0.168\times h}$$

--

Even thought this is not exactly a linear equation, it was obtained using linear regression, and we will see later on how we reached this formula. 

---

# The Basic Regression Model: description and assumptions

At the base of linear regression, we describe the relationship between an independent variable $Y$ and a dependent variable $X$ as:

$$E(Y|X=x)=\mu_{Y|x}=\beta_0+\beta_1x$$

The regression coefficient $\beta_0$ is called the **intercept**, and $\beta_1$ is called the **slope** (why?).

--

This relationship is generalized as:

$$Y=\beta_0+\beta_1x + \epsilon$$

Where $\epsilon$ is assumed to be distributed as $\mathcal{N}(0, \sigma_\epsilon)$, we will also assume it is **homoscedastic**.

--

This model is called a **simple linear regression model**.

   * Only one independent variable (only a single $x$, aka regressor)

--

Note that:

$$E(Y|x)=E(\beta_0 + \beta_1x+\epsilon)=\beta_0+\beta_1x+E\epsilon = \beta_0+\beta_1x$$

$$\operatorname{Var}(Y|x)=\operatorname{Var}(\beta_0+\beta_1x)+\operatorname{Var}(\epsilon)=\sigma^2_\epsilon$$

---

# How does the linear regression model "look like"?

The most common method to find the linear relationship is called the least squares estimate. I.e., we are looking for the line which brings to minimum the suqared errors. I.e.: 

   * The $\min\sum_i(\hat{y}_i-y_i)^2$ of the red lines in :

```{r least squares example, echo=FALSE, fig.dim=c(6,5)}
wildlife_demo <- wildlife_small %>% 
   mutate(lm_pred = exp(predict(lm(formula = log(n) ~ rounded_height, wildlife_small))))
wildlife_points <- ggplot(wildlife_demo, aes(x = rounded_height, y = n)) + 
   geom_point() + scale_y_log10() + theme_bw() + 
   stat_smooth(method = "lm", se = FALSE) + xlab("Height [k feet]") + 
   geom_segment(aes(xend = rounded_height, yend = lm_pred), color = "red", size = 1)
wildlife_points
```

---

# Finding the coefficients using the least squares method

For each observation $i$, we have: $y_i=\beta_0+\beta_1x_i+\epsilon_i$, $i=1,2,\ldots,n$

--

The sum of squares is given by

$$L = \sum_{i=1}^n{\epsilon_i^2} = \sum_{i=1}^n{(y_i-\beta_0-\beta_1x_i)^2}$$

--

To find the estimators for $\beta_0,\beta_1$ require:

$$\frac{\partial L}{\partial\beta_0}= -2\sum_{i=1}^n\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_i\right)=0$$
$$\frac{\partial L}{\partial\beta_1}= -2\sum_{i=1}^n\left(y_i-\hat{\beta}_0-\hat{\beta}_1x_i\right)x_i=0$$

---

# Finding the coefficients using the least squares method (2)

Simplifying the equations we obtain:

$$n\hat{\beta_0}+\hat{\beta}_1\sum_{i=1}^n{x_i}=\sum_{i=1}^n{y_i}$$
and

$$\hat{\beta}_0\sum_{i=1}^n{x_i}+\hat{\beta}_1\sum_{i=1}^n{x_i^2} = \sum_{i=1}^n{y_ix_i}$$

--

The solution to these equations is given by:

$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$

$$\hat{\beta}_1=\frac{\sum_i{x_iy_i} - \frac{\sum_i{y_i}\sum_i{x_i}}{n}}{\sum_i{x_i^2}-\frac{(\sum_i{x_i})^2}{n}}$$

--

The fitted line is then $\hat{y} = \hat{\beta}_0 + \hat{\beta}_1x$

---

# Finding the coefficients using the least squares method (3)

Set 

$$S_{xx}=\sum_{i=1}^n{(x_i-\bar{x})^2} = \sum_{i=1}^nx_i^2-\frac{1}{n}\left(\sum_{i=1}^nx_i\right)^2$$
$$S_{xy}=\sum_{i=1}^n{(y_i-\bar{y})(x_i-\bar{x})} = \sum_{i=1}^nx_iy_i-\frac{1}{n}\left(\sum_{i=1}^nx_i\right)\left(\sum_{i=1}^ny_i\right)$$

Then:

$$\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}$$

$$\hat{\beta}_1=\frac{S_{xy}}{S_{xx}}$$

---

# The residuals

Each observation satisfies $y_i=\hat{\beta}_0 + \hat{\beta}_1x_i+e_i, \quad i=1,2,\ldots,n$. We call $e_i$ the $i$th residual.

--

We define $SS_E$, the error sum of squares, as:

$$SS_E=\sum_{i=1}^n{e_i^2}=\sum_{i=1}^n{(y_i-\hat{y}_i)^2}$$

--

To estimate $\sigma_\epsilon^2$ we can use $SS_E$:

$$E(SS_E)=(n-2)\sigma_\epsilon^2$$

An unbiased estimator for $\sigma_\epsilon^2$ is therefore:

$$\hat{\sigma}^2=\frac{SS_E}{n-2}$$

---

# Linear Regression - Demonstration via R (1)

We'll now demonstrate how to run linear regression in R, and then continue the discussion about linear regression.

.small[
```{r linear regression demonstration}
mtcars_lm <- lm(formula = mpg ~ disp, data = mtcars)
summary(mtcars_lm)
```
]
---

# Linear Regression - Demonstration via R (2)

In this example we also use a transformation within the formula of `lm`, i.e. `log(n)`

.small[
```{r linear regression demonstration 2}
wildlife_lm <- lm(formula = log(n) ~ rounded_height, data = wildlife_small)
summary(wildlife_lm)
```
]
---

# Question

Why should you be be interested in 

   * $E\hat{\beta_0}$, 
   * $E\hat{\beta_1}$, 
   * $\operatorname{Var}{\hat{\beta_0}}$
   * $\operatorname{Var}{\hat{\beta_1}}$

--

   * Confidence intervals and hypothesis tests.
   
--
   
   * What does it mean if $\beta_1\neq0$?
   
   * What hypothesis test can we derive to test this hypothesis?

---

# Properties of the Least Squares Estimators

The estimators $\hat{\beta}_0$ and $\hat{\beta}_1$ are a result of a computation based on our sample, i.e., $\bar{y},\bar{x}, S_{xx}, S_{xy}$. 

--

Specifically, they depend on the observed $y$'s and hence they are random variables themselves. 

Both coefficients are unbiased, i.e., $E[\hat{\beta}_0]=\beta_0$ and $E[\hat{\beta}_1]=\beta_1$. 


---

# The variance of the coefficients

$$\operatorname{Var}(\hat{\beta}_1)= \frac{\sigma_\epsilon^2}{S_{xx}}$$

$$\operatorname{Var}({\hat{\beta_0}}) = \sigma_\epsilon^2\left[\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}\right]$$

The covariance of $\beta_0$ and $\beta_1$ is given by:

$$\operatorname{Cov}(\hat{\beta}_0,\hat{\beta}_1) = E(\hat{\beta}_0-E\hat{\beta_0})(\hat{\beta}_1-E\hat{\beta_1})=-\sigma_\epsilon^2\frac{\bar{x}}{S_{xx}}$$

---

# Hypothesis tests in simple linear regression

Now that we found a relationship $y=\hat{\beta}_0 + \hat{\beta}_1x$, we want to figure out, is this relationship "real"?

--

In other words, we would like to test the hypothesis:

   * $H_0:\beta_0=0$
   * $H_1:\beta_0\neq0$
   
And the hypothesis:

   * $H_0:\beta_1=0$
   * $H_1:\beta_1\neq0$

If both are rejected (especially $\beta_1$), we can say that the relationship we found is statistically significant.

--

To develop the statistical test for the coefficients we will need the following assumptions:

   * The errors are normally distributed, i.e. $\epsilon_i\sim\mathcal{N}(0,\sigma_\epsilon)$, and
   
   * The errors are homoscedastic, i.e., no matter the $x_i$, the error distribution is the same

---

# Hypothesis tests in simple linear regression (2)

Recall that

$$\hat{\beta}_1=\frac{\sum_i{x_iy_i} - \frac{\sum_i{y_i}\sum_i{x_i}}{n}}{\sum_i{x_i^2}-\frac{(\sum_i{x_i})^2}{n}}$$

Since $Y_i\sim\mathcal{N}(\beta_0+\beta_1x_i, \sigma_\epsilon^2)$, this means that $\hat{\beta}_1$ is also normally distributed (as a linear combination of normal variables).

$$\hat{\beta}_1\sim\mathcal{N}(\beta_1, \sigma_\epsilon^2/S_{xx})$$

The following statistic is distributed student's t, with $n-2$ degrees of freedom

$$T_0=\frac{\hat{\beta}_1 - 0}{\sqrt{\hat{\sigma}_\epsilon^2/S_{xx}}}$$

That is, reject $H_0$ if: 
$$t_0>t_{1-\alpha/2, n-2} \quad\text{ or }\quad t_0 < t_{\alpha/2,n-2}$$

---

# Hypothesis tests in simple linear regression (3)
.small[
A similar test can be used for $\beta_0$: $T_0=\frac{\hat{\beta}_0 - 0}{\sqrt{\hat{\sigma}_\epsilon^2\left[\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}\right]}}$
]

--

.small[
Now, we can have another look on the linear regression output.
```{r another demonstration of the wildlife regression}
summary(wildlife_lm)
```
]

---

# Analysis of Variance for Regression Significance

So far we treated the coefficients individually, however, we want a different hypothesis test which will examine the regression as a whole. 

--

The variance of the $y_i$ observations can be broken in the following manner:

$$\sum_{i=1}^n{(y_i-\bar{y})^2} = \sum_{i=1}^n{(\hat{y}_i-\bar{y})^2} + \sum_{i=1}^n{(y_i-\hat{y})^2}$$

--

The variability of the data (the LHS) is broken down to the variability explained using the regression line, and the remaining variability (the errors $e_i^2$).

--

This is sometimes noted as

$$SS_T=SS_R + SS_E$$

Where $SS_R$ has 1 degree of freedome, $SS_E$ has $n-2$ degrees of freedom, and $SS_T$ has $n-1$ degrees of freedom.

---

# Analysis of Variance for Regression

$$SS_T=SS_R + SS_E$$

The expectancy of each element is: 

   * $E[SS_E]=\sigma_\epsilon^2(n-2)$, 
   * $E[SS_R]=\sigma^2_\epsilon+\beta_1^2S_{xx}$,
   * $E[SS_T]=\sigma^2_\epsilon(n-1)$.

--

Under a null hypothesis of $H_0: \beta_1=0$ we obtain that both sum of squares are $\chi^2$ distributed.

--

Intuitively, What does it mean if $SS_R$ is large and $SS_E$ is extremely small?

I.e., if $\frac{SS_R/1}{SS_E/(n-2)}$ is very large?

--

We can use the $F$-statistic to compare the two variances

---

# Analysis of Variance for Regression Significance (2)

Then, the following statistic would be F-distributed, under the null hypothesis:

$$F_0=\frac{SS_R/1}{SS_E/(n-2)} = \frac{MS_R}{MS_E}$$
--

The intuition behind the statistic is: 

   * As the mean square error $MS_E$ decreases; and
--

   * The variance explained by the regression model $MS_R$ increases
--

   * The model is a good fit to the data 
--

   * Hence, the null hypothesis of no model, i.e., $\beta_1=0$, is rejected

--

## ANOVA (Analysis of Variance) Table

.small[
| Source of Variation | Sum of Squares | df | Mean Squares | $F_0$ |
|----------------------|-----------------|-------|:------------:|---------------------|
| Regression | $SS_R$ | $1$ | $MS_R$ | $\frac{MS_R}{MS_E}$ |
| Error | $SS_E$ | $n-2$ | $MS_E$ |  |
| Total | $SS_T$ | $n-1$ |  |  |
]

`summary(wildlife_lm)`

---

# Confidence intervals

As with any parameter, we can compute confidence intervals for $\beta_0$ and $\beta_1$:

$$\beta_1\in\hat{\beta}_1\pm t_{\alpha/2,n-1}\sqrt{\frac{\sigma_\epsilon^2}{S_{xx}}}$$

$$\beta_0\in\hat{\beta}_0\pm t_{\alpha/2, n-2}\sqrt{\frac{1}{n} + \frac{\bar{x}^2}{S_{xx}}}$$

---

# Prediction intervals

We would like to use the linear regression model for prediction of new values.

--

Given $x_0$, our prediction for $Y_0$ is

$$\hat Y_0 = \hat\beta_0 + \hat\beta_1x_0$$

--

This is a point estimate, however, we are also interested in a **prediction interval**, i.e., where would a new observation lie in probability of 95%.

--

Observe that the error of the new observation is: $e_p=Y_0-\hat Y_0$.

--

The mean of the error is 0 and the variance of the error is:

$$\operatorname{Var}(e_p)=\operatorname{Var}(Y_0-\hat Y_0)=\sigma_\epsilon^2\left[1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}}\right]$$

Hence:

$$Y_0\in \hat{y}_0\pm t_{\alpha/2,n-2}\sqrt{\hat\sigma_\epsilon^2\left[1 + \frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}}\right]}$$

---

# Prediction intervals - illustration

The grey band shows confidence intervals around $Y_0|X=x$ and the outer lines show the prediction intervals
.small[
```{r exhibiting prediction intervals, fig.dim=c(6, 4)}
wildlife_predicted <- predict(wildlife_lm,
                              newdata = 
                                 tibble(rounded_height = seq(0, 25, by = 0.25)),
                               interval = "prediction") %>% 
   data.frame() %>% 
   mutate(rounded_height = seq(0, 25, by = 0.25))
```
```
ggplot(wildlife_small, aes(x = rounded_height, y = n)) + 
   geom_point() + theme_bw() + 
   xlab("Height [k feet]") + 
   geom_line(data = wildlife_predicted, 
             inherit.aes = T, aes(x = rounded_height, y = exp(lwr))) +
   geom_line(data = wildlife_predicted, 
             inherit.aes = T, aes(x = rounded_height, y = exp(upr))) +
   scale_y_log10() +
   stat_smooth(method = "lm")
```
]
.right-plot[
```{r the actual plot, echo=FALSE, fig.dim=c(6,6)}
ggplot(wildlife_small, aes(x = rounded_height, y = n)) + 
   geom_point() + theme_bw() + 
   xlab("Height [k feet]") + 
   geom_line(data = wildlife_predicted, 
             inherit.aes = T, aes(x = rounded_height, y = exp(lwr)), color = "red") +
   geom_line(data = wildlife_predicted, 
             inherit.aes = T, aes(x = rounded_height, y = exp(upr)), color = "red") +
   scale_y_log10() +
   stat_smooth(method = "lm")
```
]

---

# Would linear regression fit here?
.small[
We mentioned these assumptions to linear regression: 

   * Linear relationship
   * Normal error $\mathcal{N}(0, \sigma_\epsilon^2)$
   * Homoscedastity
]

--

.small[
Which of the following violates these assumptions? Can you think of a transformation that would fix the problem?
]

```{r linear regression fits, echo=FALSE, fig.dim=c(7,4)}
countdown::countdown(minutes = 5)
linear_reg_examples <- tibble(x = seq(0,10, by = 0.1),
                              f1 = x + rnorm(NROW(x), mean = 0, 1),
                              f2 = x + rnorm(NROW(x), mean = 0, sd = x/10),
                              f3 = 5 + sin(x) + rnorm(NROW(x), mean = 0, sd = 0.3),
                              f4 = exp(x)/5000 + rnorm(NROW(x), mean = 0, sd = 0.3),
                              f5 = runif(NROW(x))*10,
                              f6 = 0.3*(x-5)^2 + rnorm(NROW(x))) %>% 
   pivot_longer(cols = -x, names_to = "func", values_to = "value")

ggplot(linear_reg_examples, aes(x, value)) + 
   geom_point() + 
   facet_wrap(~func) + 
   theme_bw()
```

---

# Coefficient of determination $R^2$

We would like to measure the effect size of the regression. One possibility to measure the effect size is to use $R^2$:

$$R^2=\frac{SS_R}{SS_T}=1-\frac{SS_E}{SS_T}$$

Since $SS_T=SS_R + SS_E$, and all sizes are non negative:

$$0\leq R^2 \leq1$$

As the fit is better, $R^2$ increases.

--

Next week we will also dive a bit deeper into transformations and how to utilize them, and talk about correlation between variables.

We will talk about multiple linear regression, and discuss some cavets of $R^2$, of overfitting, and how to overcome them.
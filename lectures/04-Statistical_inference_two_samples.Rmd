---
title: "Goodness of Fit, Statistical inference for Two Samples, and Independence"
subtitle: ""
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: xaringan-themer.css
---

```{r setup, include=FALSE}

library(xaringanthemer)

extra_css <- list(
   ".medium" = list("font-size" = "85%",
                    "code-font-size" = "85%"),
  ".small" = list("zoom" = "70%"),
  ".extra-small" = list("font-size" = "50%",
                        "code-font-size" = "50%"),
  ".tiny" = list("font-size" = "50%",
                 "code-font-size" = "50%",
                 "zoom" = "50%"),
  ".full-width" = list(
    display = "flex",
    width   = "100%",
    flex    = "1 1 auto"
  )
)

style_mono_accent(
   header_font_google = google_font("Open Sans"),
   text_font_google   = google_font("Rubik", "300", "300i"),
   code_font_google   = google_font("Fira Mono"),
   extra_css = extra_css,
   header_h1_font_size = "2.3rem"
   
)

theme_xaringan()

options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
```

# Reminder from previous lecture

   * Hypothesis tests (e.g., of proportion in the attractive flu shot experiment)
   
   * Type-I and Type-II errors $\alpha=P(H_1|H_0), \beta=P(H_0|H_1)$
   
   * General framework for hypothesis testing (the 8 steps)
   
      * Parameter -> Null hypothesis -> Alternative -> Significance -> Statistic -> Rejection criteria
      * Sample, computation -> Decision
   
   * P-value and the relationship to confidence intervals
   
   * Designing the sample size for a desired power $1-\beta$
   
   * Goodness of fit
   
   * Data munging (`select`, `mutate`, `filter`, `group_by`, `summarize`, etc.)

---

# Today

   * Review of survey results and what we plan to do to improve the course

   * Goodness of fit hypothesis tests (comparing two distribution)
   
   * Words of caution about HARKing, multiple comparisons, abusing hypothesis tests
   
   * Two sample hypothesis tests
   
   * Self-exercise
   
---

# Goodness of fit - motivation (1/2) מבחן טיב התאמה

Sometimes we are interested in the form of the distribution, and not just the parameter.

Why?
.tiny[
```{r ipf men vs woman read data, echo=FALSE}
set.seed(0)
ipf <- read_csv("data/ipf_lifts.csv", col_types = cols()) %>% 
   select(sex, best3deadlift_kg) %>% 
   filter(best3deadlift_kg > 0) %>% 
   group_by(sex) %>% 
   sample_n(500)
```
]
```{r ipf distribution, fig.dim=c(5, 5)}
ggplot(ipf, aes(best3deadlift_kg)) + 
   geom_density() +
   ggtitle("Distribution of the deadlift weight of 1000 atheletes")

```

Is this a normal distribution?

---

# Goodness of fit - motivation (2/2)

.small[
```{r ipf distribution geneder split, fig.dim=c(5,5)}
ggplot(ipf, aes(best3deadlift_kg, color = sex)) + 
   geom_density() +
   ggtitle("Distribution of the deadlift weight of 1000 atheletes", 
           subtitle = "Separated by gender")
```
]

   * A lot of interesting insights come from a population's distribution.
   
   * Sometimes a population's distribution is an assumption of a statistical test.
   
   * How can we quantify a "fit" to a distribution? How can we statistically test for it?


---

# Hypothesis testing - goodness of fit

Goodness of fit tests are used to test how good is the fit of our empirical distribution to that of a theoretical distribution.

--

Arrange the empirical distribution in $k$ bins, and let $O_i$ be the observed frequency in the $i$th class bin. Let $E_i$ be the expected probability. The test statistic is:

$$\chi^2_0=\sum_{i=1}^k\frac{(O_i-E_i)^2}{E_i}$$

If the population follows the hypothesized distribution, then the expression is approximately distributed $\chi^2_{k-p-1}$, where $p$ is the number of parameters of the hypothesized distribution estimated by sample statistics.

The approximation improves as $n$ increases.

--

When using the approximation, make sure that $E_i$ is "big enough" (i.e., $E_i\geq5, \forall i$)

---

# Hypothesis testing - goodness of fit - procedure

We would reject the hypothesis if $\chi_0^2>\chi_{\alpha,k-p-1}^2$.

   1. We are interested in the form of the distribution of maximum deadlift weight
   2. $H_0$: The deadlift weight is normally distributed
   3. $H_1$: The deadlift weight is not normally distributed
   4. $\alpha = 0.05$
   5. The test statistic is: $\chi^2_0=\sum_{i=1}^k{\frac{(O_i-E_i)^2}{E_i}}$
   6. Reject $H_0$ if $\chi_0^2>\chi^2_{0.05,k-p-1}$

---

# Goodness of fit - example

.tiny[
```{r}
interval_breaks <- c(0, 100, 150, 200, 250, 300, 350, 600)
sample_size <- 1000

deadlift_gfit <- ipf %>% 
   filter(!is.na(best3deadlift_kg) & best3deadlift_kg > 0) %>% 
   mutate(weight_gr = cut(best3deadlift_kg, breaks = interval_breaks))

mu <- mean(deadlift_gfit$best3deadlift_kg)
sigma <- sd(deadlift_gfit$best3deadlift_kg)

deadlift_chi_prep <- deadlift_gfit %>% 
   ungroup() %>% 
   count(weight_gr, name = "observed") %>% 
   mutate(upper_bound = interval_breaks[-1]) %>% 
   mutate(lower_bound = interval_breaks[1:7]) %>% 
   mutate(expected_prob = pnorm(q = upper_bound, mean = mu, sd = sigma)-
             pnorm(q = lower_bound, mean = mu, sd = sigma)) %>% 
   mutate(expected_prob =  expected_prob/sum(expected_prob)) %>% 
   mutate(expected = expected_prob*sample_size) %>% 
   mutate(chi_comp = (observed-expected)^2/expected)

chi2_0 <- sum(deadlift_chi_prep$chi_comp)
chi2_0
qchisq(p = 1-0.05, df = 7-2-1)
```
]

Since $\chi^2_0>\chi^2_{500, 4}=9.49$ we reject the null hypothesis. This distribution is not normal.

What is the p-value? 

--

.tiny[
```{r compute p-value}
1-pchisq(q = chi2_0, df = 7-2-1)
```
]

---

# Goodness of fit - example - using R's *chisq.test* command

.small[
```{r goodness of fit example R code native}

1-pchisq(q = chi2_0, df = 7-2-1)

chisq.test(x = deadlift_chi_prep$observed, 
           p = deadlift_chi_prep$expected_prob)
```
]


--

**Exercise**: repeat this process just for males (filtered). Do you reject the hypothesis about the normal distribution?

---

# Words of caution 

Two common mistakes when analyzing data are:

   1. HARKing (Hypothesizing After Results are Known)
   2. Using multiple comparisons without compensating for them מבחנים מרובים

In essence, Since we have a $\alpha$ error rate (e.g. 5%), when we perform 100 hypothesis tests we are bound to get 5% false positives.

--

For (1) that's something you shouldn't do! Some organizations offer *preregistration* as means of avoiding HARKing and improving research. I.e., [https://cos.io/prereg/](https://cos.io/prereg/)

--

For (2) there are procedures to control the false discovery rate (FDR). In R, there is a command called `p.adjust` for example.

---

# Abusing hypothesis tests - example (don't try this at home!)

.tiny[
```{r example for many hypothesis tests errors}
set.seed(0)

# let's get random numbers
random_normal <- matrix(rnorm(n=100*100, mean = 0, sd = 1),
                        nrow = 100, ncol = 100) %>% 
   as_tibble(.name_repair = "unique")
```
]

---

# Abusing hypothesis tests - example (don't try this at home!)

.tiny[
```{r example for many hypothesis tests errors results}
# we have 100 variables which are standard normal distributed
# let's take the one with the highest average
random_normal %>% 
   pivot_longer(cols = everything(), names_to = "variable", values_to = "value") %>%
   group_by(variable) %>% 
   summarize(mean_val = mean(value)) %>% 
   arrange(desc(mean_val))

# H_0: this variable (..78) has mu=0, H_1: otherwise

t.test(random_normal$...78)

```
]


---

# Are men and women different? ♂  ♀

Yes. Of course there are gender differences, but this is a great example to talk about!

--

Here is a research published in PLOS One, about gender differences relating to **empathy and moral cognition**:

   * Baez, Sandra, et al. "Men, women… who cares? A population-based study on sex differences and gender roles in empathy and moral cognition." *PloS one* 12.6 (2017): e0179336.
      * [https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0179336](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0179336)

--

   * The research uses statistical means to compare measures such as the **Interpersonal Reactivity Index** (IRI) to see if women score higher than men, in a **statistically significant manner**.
      * For example, figure 3. (significant sex differences in self-reported empathy) shows that women **percive** themselves as being more interpersonal than what men perceive. However, these might be driven by stereotypes, and might be apparant only in self-assesment instruments.

--
      
   * We won't go over the entire study, but I highly recommend reading it: based on a very large sample $n=10802$ and refutes some common stereotypes. (Also a misleading presentation e.g. y-axis inconsistencies, which we can learn from).
  
--

   * One very cool feature of this study is that the authors made the data available for the public [here](https://figshare.com/s/06fa32a3c57b93adc1a7).
   
---

# What is it to us?

In previous lectures we saw confidence intervals and hypothesis tests relating to a **single parameter** (single sample) פרמטר יחיד, מדגם יחיד

--

Today we will explore the comparison between **two parameters** (two samples, e.g., male-female, tall-short, before-after). שני פרמטרים, שני מדגמים

--

In the gender empathy example, let $\mu_{\text{f}}, \mu_\text{m}$ represent female and male empathy scores. We want to test the following hypothesis:

   * $H_0: \mu_\text{f}=\mu_\text{m}$
   * $H_1: \mu_\text{f}>\mu_\text{m}$
   
--
   
Then, set $\mu_\text{diff} = \mu_\text{f}-\mu_\text{m}$ and

   * $H_0: \mu_\text{diff}=0$
   * $H_1: \mu_\text{diff}>0$

--

This, more-or-less, brings us back to what we already learned last week.

---

# Difference in means - variance known - הבדל ממוצעים, שונות ידועה

Assume

   * $X_{11},\ldots,X_{1n_1}$ is a random sample from population 1.
   * $X_{21},\ldots,X_{2n_2}$ is a random sample from population 2.
   * The two populations $X_1$ and $X_2$ are independent.
   * Both populations are normally distributed.
   
Then, what is $E[\bar{X}_1-\bar{X}_2]$, and $\operatorname{Var}[\bar{X}_1-\bar{X}_2]$?

--

$$E[\bar{X}_1-\bar{X}_2] = \mu_1-\mu_2$$

$$\operatorname{Var}[\bar{X}_1-\bar{X}_2]=\operatorname{Var}(\bar{X}_1)+\operatorname{Var}(\bar{X}_2)=\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}$$ 

Using these, and the fact that the sum of two normally distributed variables is also normal we obtain...

---

# Difference in means - variance known (2)

The following quantity is normally distributed $\mathcal{N}(0,1)$:

$$Z = \frac{\bar{X}_1 - \bar{X}_2 - (\mu_1-\mu_2)}{\sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma^2_2}{n_2}}}$$

--

Null hypothesis: $H_0: \mu_1-\mu_2=\Delta_0$

Alternative Hypotheses (rejection criterion):

   * $H_1: \mu_1-\mu_2\neq\Delta_0$ $\quad(z_0>z_{\alpha/2} \text{ or } z_0<-z_{\alpha/2})$
   
   * $H_1: \mu_1-\mu_2>\Delta_0$ $\quad(z_0>z_\alpha)$
   
   * $H_1: \mu_1-\mu_2>\Delta_0$ $\quad(z_0<-z_\alpha)$

---

# Choice of sample size

Similarly to what we've seen in the last lecture, given $\alpha, \beta, \Delta$, we can determin the desired sample sizes $n_1, n_2$, by writing (example for a two sided hypothesis):

$$\beta = P(H_0|H_1)=P\left(\left. z_{\alpha/2}<\frac{\bar{X}_1-\bar{X}_2-\Delta_0}{\sqrt{\frac{\sigma_1^2}{{n_1}} + \frac{\sigma_2^2}{{n_2}}}}< z_{1-\alpha/2}\right| \mu_1-\mu_2=\Delta_0+\delta\right)$$

--

Define $\Delta = \Delta_0 + \delta = \mu_1-\mu_2$, if we play a bit with the expression in the parentheses, and using $\Phi^{-1}(\cdot)$, we can reach the following expression:

$$n\approx\frac{(z_{\alpha/2}+z_\beta)^2(\sigma_1^2+\sigma_2^2)}{(\Delta-\Delta_0)^2}$$

where $n=n_1=n_2$ and the total sample size is $2n$.

--


For a one sided alternative hypothesis, the equivalent formula would be

$$n=\frac{(z_{\alpha}+z_\beta)^2(\sigma_1^2+\sigma_2^2)}{(\Delta-\Delta_0)^2}$$

---

# Confidence intervals on a difference in means רב"ס להפרש בין תוחלות

When we discussed confidence intervals two weeks ago, we left out confidence intervals for the difference between two means, but this is actually almost the same (now that you've seen $Z$ for the difference). I.e.:

$$P\left[z_{\alpha/2}\leq\frac{\bar{X}_1-\bar{X}_2-(\mu_1-\mu_2)}{\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}}\leq z_{1-\alpha/2}\right]=1-\alpha$$

Hence, a confidence interval for $\mu_1-\mu_2$ can be obtained by using:

$$\bar{x}_1-\bar{x}_2+z_{\alpha/2}\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}\leq \mu_1-\mu_2\leq \bar{x}_1-\bar{x}_2+z_{1-\alpha/2}\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}$$

--

Or in the one-sided case:

$$\mu_1-\mu_2 \leq \bar{x}_1-\bar{x}_2+z_{1-\alpha}\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}$$ 

--

(Or the other side)
   
$$\mu_1-\mu_2 \geq \bar{x}_1-\bar{x}_2+z_{\alpha}\sqrt{\sigma_1^2/n_1+\sigma_2^2/n_2}$$ 

---

# Difference in means - variance unknown, equal variances שונות לא ידוע, שווה בשתי הקבוצות

Let's assume that $\sigma$ is unknown, but that $\sigma_1=\sigma_2=\sigma$.

We've seen that $S_i^2 = \frac{\sum_j(X_{ij}-\bar{X}_i)^2}{n_i-1}$ is an unbiased estimator for $\sigma_i^2$.

Introducing the pooled estimator of $\sigma^2$, denoted by $S^2_p$:

$$S_p^2=\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}$$

--

This is a weighted average of $S_1^2$ and $S_2^2$. 

--

Each $S_i$ contributes $n_1-1$ degrees of freedom so overall $S_p^2$ has $n_1+n_2-2$ degrees of freedom.

Given normality assumptions (or $n_1$ and $n_2$ large enough), we can use the following statistic as a student's t distribution with $n_1+n_2-2$ degrees of freedom:

$$T = \frac{\bar{X}_1-\bar{X}_2-(\mu_1-\mu_2)}{S_p\sqrt{1/n_1+1/n_2}}$$

---

# Difference in means - variance unknown, equal variances (2)

Summarizing the test, we can write:

Null hypothesis: $H_0: \mu_1-\mu_2=\Delta_0$

Test statistic: $T_0=\frac{\bar{X}_1-\bar{X}_2-\Delta_0}{S_p\sqrt{1/n_1+1/n_2}}$

Alternative hypothesis (rejection criteria):

   * $H_1:\mu_1-\mu_2\neq\Delta_0 \quad\quad(t_0>t_{1-\alpha/2,n_1+n_2-2}\text{ or } t_0<t_{\alpha/2,n-1+n_2-2})$
   * $H_1:\mu_1-\mu_2>\Delta_0 \quad\quad(t_0>t_{1-\alpha,n_1+n_2-2})$
   * $H_1:\mu_1-\mu_2<\Delta_0 \quad\quad(t_0<t_{\alpha,n_1+n_2-2})$

---

# Difference in means - variance unknown, unequal variances $(\sigma_1\neq\sigma_2)$ שונות לא ידועה, שאינה שווה בין הקבוצות

In the case that the variances are unequal and unknown, we can use the following statistic which is approximately distributed $t$ with $v$ degrees of freedom:

$$T^*_0=\frac{\bar{X}_1-\bar{X}_2-\Delta_0}{\sqrt{S_1^2/n_1+S_2^2/n_2}}$$

Where the degrees of freedom $v$ are equal:

$$v=\frac{\left(S_1^2/n_1 + S_2^2/n_2\right)^2}{\frac{(S_1^2/n_1)^2}{n_1-1}+\frac{(S_2^2/n_2)^2}{n_2-1}}$$

---

# Paired or unpaired t-test? מבחן מזווג או לא מזווג

Sometimes, data is collected in pairs, for example when an intervention plan is conducted on a group of individuals

   * We want to compare the effect of the intervention before and after. 

   * This *paired* situation may ocurr when we have multiple observations of the same group. 
   
   * We would like to match the observations to avoid differences which may occur due to variation between subjects. 

--

   * We pair the observations and conduct the statistical tests on the difference.
   
Let $(X_{11}, X_{21}),\ldots,(X_{1n}, X_{2n})$ a set of $n$ paired observations. Define $D_j=X_{ij}-X_{2j}$

--

$$\mu_D=E(X_1-X_2)=\mu_1-\mu_2$$

--

Null hypothesis: $H_0:\mu_D=\Delta_0$

Test statistic: $T_0=\frac{\bar{D}-\Delta_0}{S_D/\sqrt{n}}$

Where $S_D$ is the sample standard deviation of the differences.

---

# Example of two sample hypothesis testing - the ipf data set

We go back to the ipf data set (the power lifting competition dataset). 

What test would you use? (means paired / means unpaird / something else) for each of the following:
.small[
Test 1:
   * $H_0:$ men lift higher weights than women
   * $H_1:$ men and women lift the same weight
   
Test 2:
   * $H_0:$ Deadlift and squat weights are the same
   * $H_1:$ Deadlift weights are higher than squat weights

Test 3:
   * $H_0:$ The age of male athletes is normally distributed
   * $H_1:$ The age of male athletes is not normally distributed

Test 4:
   * $H_0:$ The athletes age and gender are statistically independent
   * $H_1:$ The athletes age and gender are not statistically independet
]

---

# Demonstrating tests in the ipf data set (test 1)

   * $H_0:$ men and women lift the same weight
   * $H_1:$ men lift higher weights than women

Unpaired t-test (note that I switched $H_0$ and $H_1$)
.tiny[
```{r read ipf and first test}
#https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-08/ipf_lifts.csv
set.seed(0)
ipf <- read_csv("data/ipf_lifts.csv", col_types = cols()) %>% 
   filter(best3squat_kg > 0) %>% 
   sample_n(1000)
   
t.test(formula = best3squat_kg ~ sex, data = ipf, alternative = "less")

# alternative method to call the function
#t.test(x = ipf$best3squat_kg[ipf$sex == "F"], y = ipf$best3squat_kg[ipf$sex == "M"], alternative = "less")
```
]

---

# Demonstrating tests in the ipf data set (test 2)

Test 2:
   * $H_0:$ Deadlift and squat weights are the same
   * $H_1:$ Deadlift weights are higher than squat weights
   
Paired t-test, because each athelete is compared to himself

.tiny[
```{r ipf paired ttest}
t.test(x = ipf$best3deadlift_kg, y = ipf$best3squat_kg, paired = TRUE, alternative = "greater")
```
]

---

# Demonstrating tests in the ipf data set (test 3 and 4)

Test 3 was:

   * $H_0:$ The age of male athletes is normally distributed
   * $H_1:$ The age of male athletes is not normally distributed
   
This is the goodness of fit test that we were discussing in the previous unit.

Test 4 was:

   * $H_0:$ The athletes age and gender are statistically independent
   * $H_1:$ The athletes age and gender are not statistically independet

The fourth test is actually a goodness of fit test for uniform distribution.

---

# Goodness of fit test for independence (contingency table)

When we have two variables and we want to examine independence between them, it is like comparing the contingencies (combinations) of the two variables, against the uniform distribution.

| Age | genderM | genderF | total |
|--------|---------|---------|--------|
| group1 | g1m | g1f | $n_{g1}$ |
| group2 | g2m | g2f | $n_{g2}$ |
| group3 | g3m | g3f | $n_{g3}$ |
| total | $n_M$ | $n_F$ | $n$ |

--

   * The observed is the count within the cell, and the expected is the product of the marginal probabilities, i.e.:
   
      * The expected of Males in group2 under the null hypothesis is:  $E_{g2m}=(n_{g2}/n)\times(n_M/n)\times n$

   * This is the case when the variables are independent, i.e., $P(X=x,Y_y)=P(X=x)P(Y=y)$

--

This results in a $\chi^2$ test with $(r-1)(c-1)$ degrees of freedom.

---

# Independence between variable via contingency table (example)
.small[
```{r contingency table independence test via chi square}
ipf_new <- ipf %>%
   filter(!is.na(age)) %>% 
   mutate(age_group = cut(age, breaks = c(0, 25, 35, 45, 55, 100))) %>% 
   count(age_group, sex) %>% 
   pivot_wider(id_cols = age_group, names_from = sex, values_from = n) %>% 
   select(2:3) %>% 
   as.matrix()
   
ipf_compare_res <- chisq.test(ipf_new)
ipf_compare_res
```
]

---

# Your turn (class exercise) - do mobile phones impact our health?

In **pairs**, try to devise an experiment plan, that would test whether **mobile phones impact our health**.
.small[
In your answer relate to the following points:

   * What do you consider as an "effect"? (i.e., what kind of health measure?)

   * How do you select and/or separate the groups participating in your experiment?
   
   * How do you neutralize other factors which might intervene with the experiment? (like selection of participants or other factors)
   
   * What would you use as a statistcal measure?
   
   * Paired or unpaired?
   
   * What sample size?
   
   * What would be the hypotheses 8-step procedure of the experiment? reminder:
      * Parameter -> Null hypothesis -> Altermative -> Significance -> Statistic -> Rejection criteria
      * Sample, computation -> Decision
]

---

# Variance of two samples (F-test)

We talked about t-test when the variance is unknown and presented two cases (with equal variance and uneqal variance). How would we decide which of the two to use? 

--

We would like to use **a statistical test** to compare the variance. One method to compare two numbers is to divide them, i.e., $\sigma_1/\sigma_2$.

--

The $F$ distribution is the ratio of two independent chi-square random variables, divided by its number of degrees of freedom, i.e.:
   
$$F=\frac{W/u}{Y/v}$$

The probability density function is given by the expression:

$$f(x)=\frac{\Gamma\left(\frac{u+v}{2}\right)\left(\frac{u}{v}\right)^{u/2}x^{(u/2)-1}}{\Gamma\left(\frac{u}{2}\right)\Gamma\left(\frac{v}{2}\right)\left[\left(\frac{u}{v}\right)x+1\right]^{(u+v)/2}}$$

---

# Illustration of the $F$ distribution
.small[
$$F=\frac{W/u}{Y/v}, \quad\mu=v/(v-2), \quad (\text{ for }v>2), \quad \sigma^2=\frac{2v^2(u+v-2)}{u(v-2)^2(v-4)}$$
]
.tiny[
```{r example F distribution, fig.dim=c(7,4)}
f_dist <- crossing(f = seq(0, 6, by = 0.1), df1 = c(1, 5, 10, 20), df2 = c(1, 5, 10, 20)) %>% 
   mutate(f_dense = pmap_dbl(.l = list(f, df1, df2), .f = df)) %>% 
   mutate(v = paste0("v=", df2),
          u = paste0("u=", df1)) %>% 
   mutate_at(vars(v,u), fct_inorder)

ggplot(f_dist, aes(x = f, y = f_dense)) + 
   geom_line() + 
   facet_grid(rows = vars(u), cols = vars(v)) + 
   theme_bw() + 
   guides(color = guide_legend("df"))

```
]

---

# Statistical hypothesis test for variance equality

Assuming two independent populations with normal distribution, then $F=\frac{S_1^2/\sigma_1^2}{S_2^2/\sigma_2^2}$ has an $F$ distribution with $u=n_1-1, v=n_2-1$ degrees of freedom.

Null hypothesis: $H_0: \sigma_1^2=\sigma_2^2$

Test statistic: $F_0 = \frac{S_1^2}{S_2^2}$

Alternative hypothesis (rejection criteria):

   * $H_1:\sigma_1^2\neq\sigma_2^2 \quad(f_0>f_{1-\alpha/2, n_1-1, n_2-1} \text{ or } f_0<f_{\alpha/2,n_1-1, n_2-1})$
   
   * $H_1:\sigma_1^2>\sigma_2^2 \quad(f_0>f_{1-\alpha,n_1-1, n_2-1})$
   
   * $H_1:\sigma_1^2<\sigma_2^2 \quad(f_0<f_{\alpha,n_1-1, n_2-1})$

---

# Example for F statistic variance test

.small[
```{r exercise variance between genders}
ipf %>% 
   group_by(sex) %>% 
   summarize_at(vars(contains("best3")), ~{(sd(., na.rm = T))^2})
```
]

It's pretty clear that males have a much higher variance, lets test this in an F test.

.small[
```{r test variance between genders}
var.test(formula = best3squat_kg ~ sex, data = ipf, ratio = 1, alternative = "less")
```
]

---

# Inference on two population proportions

We consider the case of two binomial parameters $p_1, p_2$. Let $X_1, X_2$ represent the number of successes in each sample. $\hat{P}_i=X_i/n_i$, have approximately normal distributions.

$$Z=\frac{\hat{P}_1-\hat{P}_2-(p_1-p_2)}{\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}}$$
Is distributed approximately as $Z\sim\mathcal{N}(0,1)$.

--

Under the null hypothesis $H_0: p_1=p_2=p$ we have:

$$Z = \frac{\hat{P}_1-\hat{P}_2}{\sqrt{p(1-p)(1/n_1 + 1/n_2)}}$$

--

Where an estimator to $p$ is given by:

$$\hat{P}=\frac{X_1+X_2}{n_1+n_2}$$

---

# The test procedure for comparing two population proportions

Null hypothesis: $H_0: p_1=p_2$

Test statistic: $Z_0=\frac{\hat{P}_1-\hat{P}_2}{\sqrt{\hat{P}(1-\hat{P})(1/n_1 + 1/n_2)}}$

Alternative hypothesis (rejection criteria):

   * $H_1:p_1\neq p_2 \quad(z_0>z_{1-\alpha/2} \text{ or } z_0<z_{\alpha/2})$
   
   * $H_1:p_1>p_2 \quad (z_0>z_{1-\alpha})$
   
   * $H_1:p_1<p_2 \quad (z_0<z_{\alpha})$
   
---

# Setting the sample sizes when comparing two population proportions

Very similar to what we've shown in the last lecture for one sample, but with a slightly different computation for the standard deviation under $H_1$. For example, in the two sided case we have:

$$\beta=\Phi\left[\frac{z_{1-\alpha/2}\sqrt{\bar{p}\bar{q}(1/n_1+1/n_2)}-(p_1-p_2)}{\sigma_{\hat{P}_1-\hat{P}_2}}\right]-\Phi\left[\frac{z_{\alpha/2}\sqrt{\bar{p}\bar{q}(1/n_1+1/n_2)}-(p_1-p_2)}{\sigma_{\hat{P}_1-\hat{P}_2}}\right]$$

--

With $\bar{p}=\frac{n_1p_1+n_2p_2}{n_1+n_2}, \bar{q}=\frac{n_1(1-p_1)+n_2(1-p_2)}{n_1+n_2}$ and

$$\sigma_{\hat{P}_1-\hat{P}_2}=\sqrt{\frac{p_1(1-p_1)}{n_1} + \frac{p_2(1-p_2)}{n_2}}$$

--

We can obtain the suggested sample (or power) using `pwr::pwr.2p.test` or `pwr::pwr2p2n.test`.

.small[
```{r demonstration for sample size using pwr}
pwr::pwr.2p2n.test(h = pwr::ES.h(p1 = 0.2, p2 = 0.3),
                   n1 = 150, n2 = NULL,
                   sig.level = 0.05,
                   power = 0.8,
                   alternative = "less")
```
]

---

# Effect Size

We discussed p-value as the extent to which a statistical finding is significant. However, it is not the sole measure for the strength of a statistical finding.

--

In this context, see the ASA statement on $p$-Values [here](https://amstat.tandfonline.com/doi/full/10.1080/00031305.2016.1154108)

--

**Effect size** measures the magnitude of a phenomena. Effect size is a generic name for various measures such as:

   * $R^2$ in linear regression
   
   * $\rho$ Pearson correlation coefficient between two variables
   
   * Cohen's $d$ which relates to the difference between means (which we will now discuss)
   
   * Many more

---

# Effect Size - Cohen's $d$

The difference between two means divided by standard deviation, i.e.:

$$d=\frac{\bar{X}_1-\bar{X}_2}{S_p}$$

Where $S_p$ is the pooled standard deviation:

$$S_p=\sqrt{\frac{(n_1-1)S_1^2+(n_2-1)S_2^2}{n_1+n_2-2}}$$
.small[
```{r example for cohens d}
effsize::cohen.d(formula = best3squat_kg ~ sex, data = ipf)
```
]
---
title: "Point Estimation Methods and Intervals"
subtitle: ""
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}
.remark-slide-content {
  font-size: 28px;
  padding: 20px 80px 20px 80px;
}
.remark-code, .remark-inline-code {
  background: #f0f0f0;
}
.remark-code {
  font-size: 24px;
}
.huge .remark-code { /*Change made here*/
  font-size: 200% !important;
}
.tiny .remark-code { /*Change made here*/
  font-size: 50% !important;
}

.small .remark-code {
   font-size: 75% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

small-slide {
   font-size: 70% !important;
}

.gray-out {
   color: #BEBEBE;
}

```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
```


# Reminder from previous lecture

Last lesson we talked about:

--

   * How data analysis is conducted (import -> tidy -> transform, visualize, model -> communicate)

--

   * About R, very broadly (RStudio, Rmd, scripts, functions, packages)
   
--

   * What a tidy data set looks like (rows = observations, columns = variables, aka features)
   
--

   * Variable types (numeric, date, logical, factor - category, ordinal)
   
--

   * We did a lab about COVID19 mobility data with some visualizations
   
--

   * Descriptive statistics (average, median, standard deviation)
   
---

# What we will discuss today

   * Population versus sample
   
   * Point estimates
   
   * Bias and variance of point estimates
   
   * Estimation using maximum likelihood
   
   * Estimation using the moments method
   
   * Confidence intervals (incl. examples)
   
---

# Population versus sample

We would like information about a **population**, however:

   * Most of the times it is impossible to get to the whole population (why? can you think of examples where we *can* reach the whole population?)

--

   * Instead of using a whole population we use a **sample** of the population. For example:
   
      * Probability based: random, cluster, stratified.
      
      * Non-probability: Snowball / respondent-driven-sample.
      
--

   * **The problem** - when we use a sampling process we will never be accurate.

   * We use statistics to bridge this gap: understand our errors, understand our uncertainty
  
---

# Parameters, estimators, and estimates

   * Example here: [https://sarid.shinyapps.io/population_vs_sample/](https://sarid.shinyapps.io/population_vs_sample/)

--

   * What are point estimates, i.e.:
   
      * $\theta$ is a *population parameter* (e.g., $\mu$)
      
      * Estimated by the statistic $\hat{\Theta}$, a *point estimator* (e.g., $\bar{X}=\frac{1}{n}\sum{x_i}$)
      
      * When it is computed (from a sample), it is called a *point estimate* 
      
---

# An estimator is a **random variable**

   * An estimator is the result of sampling

--

   * An estimator is random variable

--
   
   * For example, $\bar{X}$
   
      * We can represent its expectancy and variance: $E[\bar{X}] = \mu, \operatorname{Var}(\bar{X}) = \frac{\sigma^2}{n}$
      
      * If $n$ is "large enough" (i.e., $n\geq30$) is also approximately normally distributed (central limit theorem)
      
--

   * We will use this property **A lot** during this course
      
---

# Contents

   * .gray-out[Population versus sample]
   
   * .gray-out[Point estimates]
   
   * Bias and variance of point estimates
   
   * Estimation using maximum likelihood
   
   * Estimation using the moments method
   
   * Confidence intervals (incl. examples)
   
---

# Desired properties of point estimators
   
   * Unbiased: $E\hat{\Theta} - \theta = 0$
   
      * In the case of $\bar{X}$ the bias is indeed 0, i.e.: $E\bar{X}-\mu=0$

--

   * Low variance: $\operatorname{Var}(\hat{\Theta})$ as low as possible

--
   
I've shown that: 

   * The average $\bar{X}=\sum_{i=1}^n{x_i}$ is unbiased: 
      * $E[\bar{X}] = \mu$
      
   * Its variance is $V(\bar{X})=\frac{\sigma^2}{n}$
   
   * In fact, it is the *Minimum Variance Unbiased Estimate* (proof out of scope)
   
--
   
   * Question: let's say we sampled $n$ observation but only use $x_1$ as our estimator. 
   
      * What is its bias? 
      
      * What is its variance?

---

# Another example $S^2$
      
   * Sample variance: $S^2=\frac{\sum_{i=1}^n(x_i-\bar{X})^2}{n-1}$ is an **unbiased** estimator of $V(X)=\sigma^2$
   
      * $V(X)=E(X-E[X])^2=EX^2-(EX)^2$
   
   * Standard deviation: $S$ is a **biased** estimator for $\sigma$

---

# Why $S^2$ is an unbiased estimator to $\sigma^2$?

$S^2$ is unbiased:

$$ES^2=\frac{1}{n-1}E{\sum_{i=1}^n(x_i-\bar{X})^2}=\frac{1}{n-1}E{\sum_{i=1}^n(x_i^2+\bar{X}^2-2\bar{X}x_i)}=$$

--

$$=\frac{1}{n-1}E\left(\sum_{i=1}^nx_i^2-n\bar{X}^2\right)=\frac{1}{n-1}\left[\sum_{i=1}^n{Ex_i^2-nE(\bar{X}^2)}\right]$$

--

Using the fact that $E(x_i^2)=\mu^2+\sigma^2$ and that $E(\bar{X}^2)=\mu^2+\sigma^2/n$ we have:

--

$$E(S^2)=\frac{1}{n-1}\left[\sum_{i=1}^n(\mu^2+\sigma^2)-n(\mu^2+\sigma^2/n)\right]=\sigma^2$$

$$\square$$

---

# Why is $S$ a biased estimator to $\sigma$?

We need to show that $ES\neq\sigma$. Let 

$$S=\sqrt{\sum_{i=1}^n{\frac{(x_i-\bar{X})^2}{n-1}}}$$

--

Consider that $0<\operatorname{Var}(S)=E[S^2]-(E[S])^2$ (this is true for any RV, specifically $S$ in this case)

--

Hence

$$(ES)^2<E[S^2] \Leftrightarrow ES<\sqrt{E[S^2]}=\sigma$$ 

$$\square$$

In certain cases, we can directly compute this bias (i.e. what is $ES-\sigma$), for example, if we assume $X\sim N(\mu,\sigma)$, then:

$$\sigma-E(S)\cong\frac{\sigma}{4n}$$ (see [here](https://stats.stackexchange.com/questions/11707/why-is-sample-standard-deviation-a-biased-estimator-of-sigma)).

---

# Short quiz - identify the distributions

Before we continue, **identify the distributions using the plot**, and write them down on a piece of paper, in pairs.

Here are your options: Normal, Chi-square, Bernoulli, Exponential, Poisson.

```{r recognize the distribution quiz, echo=FALSE, fig.dim=c(8, 6)}

tibble(x = seq(-25, 25, 1)) %>% 
  mutate(dist1 = dexp(x, rate = 1/5),
         dist2 = dnorm(x, mean = 1, sd = 3),
         dist3 = dpois(x, lambda = 5),
         dist4 = dchisq(x, df = 5)) %>% 
  pivot_longer(cols = -x, names_to = "distribution", values_to = "y") %>% 
   ggplot(aes(x=x, y=y, fill = distribution)) + 
   geom_col() + 
   facet_wrap(~distribution) + 
   coord_cartesian(xlim = c(-10, 25)) + 
   theme_bw()

```

---

# The Central Limit Theorem

If $\bar{X}$ is the mean of a random sample of size $n$ taken from a population with mean $\mu$ and finite variance $\sigma^2$, then the limiting form of the distribution of 

$$Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$$

As $n\rightarrow\infty$ is the standard normal distribution $N(0,1)$

For most purpuses, $n\geq30$ is considered "large enough" (as a rule-of-thumb). 

Lets look at an example of exponential distribution: $\operatorname{Exp}(\lambda=1)$. **What are $\mu$ and $\sigma$?**

---

# The distribution of $\bar{X}$ with varying $n$

With $\operatorname{Exp}(\lambda=1)$ we have $\mu=\sigma=1/\lambda=1$

Try increasing `sample_size` and see what happens to the chart.

--

.tiny[
```{r example how a sum of exp becomes normal, message=FALSE, warning=FALSE}
sample_size <- 1
lambda <- 1

rv_binom <- matrix(rexp(n = sample_size*1000, rate = lambda), nrow = sample_size, ncol = 1000) %>% 
  as_tibble(.name_repair = "unique") %>% 
  pivot_longer(cols = everything(), names_to = "var", values_to = "value") %>% 
  group_by(var) %>% 
  summarize(mean = mean(value)) %>% 
  mutate(standardized = (mean - 1/lambda)/((1/lambda)/sqrt(sample_size)))

ggplot(rv_binom, aes(x = standardized)) + 
  geom_histogram(bins = 100) + 
  theme_bw()

```
]

---

# Contents

   * .gray-out[Population versus sample]
   
   * .gray-out[Point estimates]
   
   * .gray-out[Bias and variance of point estimates]
   
   * Estimation using maximum likelihood
   
   * Estimation using the moments method
   
   * Confidence intervals (incl. examples)
   
---

# Methods of point estimation

So far we discussed some estimators (i.e., $\bar{X}, S^2$), and the desired properties they hold. But how can we find additional estimators (new statistics)?

We will show two methods:

   * Maximum Likelihood Estimation (MLE)
   
   * Moment method

---

# Maximum Likelihood Estimation (MLE)

An important and very common approach to solving estimation problems in statistics. The idea is as follows:

If you want to estimate some population parameter $\theta$, use the most **likely** value. 

--

Let $f(x;\theta)$ represent the density function of $X$. Given a sample $x_1,\dots,x_n$ in which $x_i$'s are independent, we can write the liklihood of the sample:

$$L(\theta)=f(x_1,\ldots,x_n;\theta)=f(x_1;\theta)\times\ldots\times f(x_n;\theta)=\prod_{i=1}^nf(x_i;\theta)$$

--

   * The product (multiplication) can be used thanks to the assumption of independence of the sampled observations.
   
   * $L(\theta)$ is a function of $\theta$ (only) once the sample has been set.

   * In the discrete case we will use $P(X_1 = x_1,\ldots,X_n=x_n;\theta)$. The likelihood is the same as the probability of obtaining the sample.

---

# MLE example - Poisson distribution (discrete)

The Poisson distribution is used to represent a counting processes. I.e., the number of accumulated events is distributed Poisson.

--

$$P(X=x)=\frac{\lambda^xe^{-\lambda}}{x!}$$

--
.small[
```{r demonstration poisson, fig.dim=c(7, 3)}

poiss_demonstration <- crossing(x = 0:25, pois_rate = c(0.5, 1, 2, 10)) %>% 
   mutate(dpois = map2_dbl(x, pois_rate, dpois))

ggplot(poiss_demonstration, aes(x = x, y = dpois, fill = factor(pois_rate))) +
   geom_col() + 
   facet_wrap(~pois_rate) + 
   theme_bw()

```
]

---

# MLE example - Poisson distribution (discrete) - cont.

Assume we sample the number of arrivals to the hospital during $n$ weekdays: $x_1,\ldots,x_n$. The number of arrivals is distributed with a $\text{Poisson}(\lambda)$ distribution.

--

Then, the likelihood is:

$$L(\lambda) = \prod_{i=1}^n{(\lambda^{x_i}e^{-\lambda})/x_i!}$$

--

Taking logarithm we have:

$$\log L(\lambda)=\sum_{i=1}^n{\left(x_i\log\lambda-\lambda\log(e)-\log(x_i!)\right)}$$

--

Now we require an extremum, i.e. $d\log L(\lambda)/d\lambda=0$:

$$\sum_{i=1}^n{x_i/\lambda^*}-n=0 \Longrightarrow \lambda^*=\bar{X}$$

--

This is indeed a maximum $\frac{d^2\log{L(\lambda)}}{d\lambda^2}=-\left(\sum_{i=1}^n{\frac{x_i}{\lambda}}\right)<0$

---

# MLE example - Bernulli distribution

Let's assume we conducted $n$ experiments each with probability $p$ for success and $q=1-p$ for failure. These are Bernoulli i.i.d variables.

$$B_i=\left\{\begin{array}{ll}1 & \text{w.p. } p\\ 0 & \text{w.p. } 1-p\end{array}\right.$$

Assume we got $v$ success, $n-v$ failures. 

   * What is the Liklihood?
   * What is the log-Liklihood?
   * Find the optimal $\hat{p}$.

---

# Maximum Liklihood Estimate - Normal distribution

The previous examples dealt with discrete distributions. What happens in the continuous case?

--

Assume $X_1,\ldots,X_n\sim \mathcal{N}(\mu,\sigma)$ i.i.d distributed. We sample $x_1,\ldots,x_n$ and we are looking for an MLE for $\mu$ and $\sigma^2$:

--

The likelihood:

$$L(\mu)=\prod_{i=1}^n\left(\frac{e^{\frac{-(x_i - \mu)^2}{2 \sigma^2}}}{\sqrt{2\pi}\sigma}\right)=\frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2}\sum_{i=1}^n{\left(\frac{x_i-\mu}{\sigma}\right)^2}\right]}$$

--

Taking $\log$ we get:

$$\log L(\mu,\sigma^2)=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\sigma^2)-\frac{1}{2}\sum_{i=1}^n\left(\frac{x_i-\mu}{\sigma}\right)^2$$

--

$$\frac{\partial\log L(\mu,\sigma^2)}{\partial \mu}=\sum_{i=1}^n{\frac{x_i-\mu}{\sigma}} \Longrightarrow \mu^*=\bar{X}$$

---

# Maximum Liklihood Estimate - Normal distribution - cont.

Now, derivative by $\sigma$ we obtain:

$$\frac{\partial\log L(\mu,\sigma^2)}{\partial (\sigma^2)}=-\frac{n}{2}\frac{1}{\sigma^2}+\frac{1}{2}\frac{1}{\sigma^4}\sum_{i=1}^n(x_i-\mu)^2$$

From here we get:

$$(\sigma^*)^2=\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2$$

--

As you have seen, $(\sigma^*)^2$ is a **biased** estimator to the variance (the unbiased estimator $s^2$ had $n-1$ in the denominator).

---

# Contents

   * .gray-out[Population versus sample]
   
   * .gray-out[Point estimates]
   
   * .gray-out[Bias and variance of point estimates]
   
   * .gray-out[Estimation using maximum likelihood]
   
   * Estimation using the moments method
   
   * Confidence intervals (incl. examples)
   
---

# Method of Moments Estimation

In this method, we use knowledge about the moments of the distribution (i.e. $E[X], E[X^2], E[X^3], \ldots$), express the parameters as functions of these moments, and then use the sample moments to compute our estimator.

Let $f(x)$ be a density function for $X_1,\ldots,X_n$, then:

| Moment | Continuous | Discrete | Sample |
|--------|------------|----------|--------|
| $E[X]$ | $\int{xf(x)dx}$ | $\sum_k{kP(X=k)}$ | $(1/n)\sum_{i=1}^{n}{x_i}$ |
| $E[X^2]$ | $\int{x^2f(x)dx}$ | $\sum_k{k^2P(X=k)}$ | $(1/n)\sum_{i=1}^{n}{x_i^2}$ |
| $E[X^m]$ | $\int{x^mf(x)dx}$ | $\sum_k{k^mP(X=k)}$ | $(1/n)\sum_{i=1}^{n}{x_i^m}$ |

The sample mean $\bar{X}=(1/n)\sum_{i=1}^n{x_i}$ is the moment estimator of the population mean.

---

# Method of Moments - Example - the Normal disribution

The first moment is $E[X]=\mu$, hence $\hat{\mu}=\bar{X}$ is the moment estimator for the population mean.

--

The second moment is $E[X^2] = \mu^2 + \sigma^2$ require:

$$\mu^2 + \sigma^2 = \frac{1}{n}\sum_{i=1}^n{x_i^2}$$

--

Set $\mu^2=\left(\frac{1}{n}\sum_{i=1}^n{x_i}\right)^2$

--

$$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n{x_i^2} - \left(\frac{1}{n}\sum_{i=1}^n{x_i}\right)^2=\frac{\sum{x_i^2}-(\sum{x_i})^2/n}{n}=\frac{\sum_{i=1}^n(x_i-\bar{X})^2}{n}$$

Again, this is a biased estimator for $\sigma^2$, usually we will use $s^2$ seen earlier.

---

# Contents

   * .gray-out[Population versus sample]
   
   * .gray-out[Point estimates]
   
   * .gray-out[Bias and variance of point estimates]
   
   * .gray-out[Estimation using maximum likelihood]
   
   * .gray-out[Estimation using the moments method]
   
   * Confidence intervals (incl. examples)
   
---

# Statistical Intervals

We discussed point estimates, however

   * Even if everything works "properly" (a random sample, unbiased estimator), it is unlikely that we will reach the exact parameter value
   
   * As the sample increases accuracy improves; but

--
   
   * Sometimes we are interested in a *Confidence Interval*
   
   * An interval of the form $\hat{\Theta}_l < \theta < \hat{\Theta}_u$ where 

--
   
   * The lower and upper bounds $\hat{\Theta}_l, \hat{\Theta}_u$ depend on the statistic $\hat{\Theta}$

--

In a probabalistic notation, we are looking for $\hat{\Theta}_l, \hat{\Theta}_u$ such that:

$$P(\hat{\Theta}_l < \theta < \hat{\Theta}_u) = 1-\alpha$$

For $\alpha\in(0,1)$. For example, when we set $\alpha=0.05$, we call this a 95% confidence interval for $\theta$.

---

# Sanity check

**What would be a 100% confidence interval**? 

   * I.e., what would be $\hat{\Theta}_l, \hat{\Theta}_u$ such that:

$$P(\hat{\Theta}_l < \theta < \hat{\Theta}_u) = 1$$

--

Setting $\hat{\Theta}_l=-\infty, \hat{\Theta}_u=\infty$ gives us a 100% confidence interval (i.e., the $\theta$ is a real number).

---

# Confidence Interval for Normal Distribution with Known Variance

We previously mentioned the central limit theorem and that 

$$Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$$

Is normally distributed as $n\rightarrow\infty$. Hence:

$$P(z_{\alpha/2} < Z < z_{1-\alpha/2}) = 1-\alpha$$

--

$$P(z_{\alpha/2} < \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} < z_{1-\alpha/2}) = 1-\alpha$$

--

Using the fact that for the normal distribution $z_{1-\alpha/2}=-z_{\alpha/2}$:

$$P(\bar{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} < \mu < \bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}) = 1-\alpha$$

---

# Example - determining the sample size from a desired confidence range

If we want to have a confidence interval with a range not exceeding $\pm r$, we can use:

$$\bar{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}-\left(\bar{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right) \leq 2r$$

--

Then,

$$\sqrt{n}\geq z_{\alpha/2}\frac{2\sigma}{2r}$$

--

$$n\geq \left(z_{\alpha/2}\frac{\sigma}{r}\right)^2$$

Ever wondered why surveys have $n=500$ respondents? it comes from a $\pm 4.4\%$ margin of error with a $95\%$ confidence interval (using a binomial distribution). 

---

# Confidence Interval for Normal Distribution with Unknown Variance

In this case, we use our estimator $S$ to compute our statistic and confidence interval.

$$T = \frac{\bar{X}-\mu}{S/\sqrt{n}}$$

The statistic $T$ has a student's t-distribution with $n-1$ degrees of freedom. I.e.:

$$P(-t_{\alpha/2,n} < T < t_{\alpha/2,n})=1-\alpha$$

.center[
```{r example for the t distribution, echo = F, fig.dim=c(7,4)}

t_dist_example <- tibble(x = -seq(-10, 10, 0.1)) %>% 
   mutate(t01 = dt(x, df = 1),
          t05 = dt(x, df = 5),
          t10 = dt(x, df = 10),
          norm = dnorm(x, mean = 0, sd = 1)) %>% 
   pivot_longer(cols = -x, names_to = "distribution", values_to = "y")

ggplot(t_dist_example, aes(x, y, color = distribution, linetype = distribution)) + 
   geom_line(size = 0.8) + 
   theme_bw()

```
]

---

# Summary

   * .gray-out[Population versus sample]
   
   * .gray-out[Point estimates]
   
   * .gray-out[Bias and variance of point estimates]
   
   * .gray-out[Estimation using maximum likelihood]
   
   * .gray-out[Estimation using the moments method]
   
   * .gray-out[Confidence intervals (incl. examples)]
   
Next week we will continue our discussion of confidence intervals and see the implementation in R.
   
---
---
title: "Point Estimation Methods and Intervals"
subtitle: ""
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "Last updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: xaringan-themer.css
---

```{r setup, include=FALSE}

library(xaringanthemer)

extra_css <- list(
   ".medium" = list("font-size" = "85%",
                    "code-font-size" = "85%"),
  ".small" = list("font-size" = "70%",
                  "code-font-size" = "70%"),
  ".extra-small" = list("font-size" = "50%",
                        "code-font-size" = "50%"),
  ".full-width" = list(
    display = "flex",
    width   = "100%",
    flex    = "1 1 auto"
  )
)

style_mono_accent(
   header_font_google = google_font("Open Sans"),
   text_font_google   = google_font("Rubik", "300", "300i"),
   code_font_google   = google_font("Fira Mono"),
   extra_css = extra_css,
   header_h1_font_size = "2.3rem"
   
)

theme_xaringan()

options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
```


# Reminder from previous lecture

Last lesson we talked about:

--

   * How data analysis is conducted (import -> tidy -> transform, visualize, model -> communicate)

--

   * About R, very broadly (RStudio, Rmd, scripts, functions, packages)
   
--

   * What a tidy data set looks like (rows = observations, columns = variables, aka features)
   
--

   * Variable types (numeric, date, logical, factor - category, ordinal)
   
--

   * We did a lab about COVID19 mobility data with some visualizations
   
--

   * Descriptive statistics (average, median, standard deviation)
   
---

# Also discussed sample (מדגם) versus population (אוכלוסיה)

   * We would like information about a **population**.
   
   * Often times it is impossible to get to the whole population, so we use a **sample**.

   * Parameters (פרמטרים): $\Theta$, $p$, versus Estimators (אמדים): $\hat{\theta}, \hat{p}$.
   
   * Expectation (תוחלת): $EX$, versus average (ממוצע) $\bar{X}=\sum{x_i}/n$.
   
   * Variance (שונות): $\operatorname{Var}(X)$, versus sample variance (שונות מדגמית): $S^2=\frac{\sum{(X_i-\bar{X})^2}}{n-1}$.
   
   * **The problem** - when we use a sampling process we will never be accurate.

   * We use statistics to bridge this gap: understand our errors, understand our uncertainty

---

# What we will discuss today
   
   * Methods for point estimates (אמידה נקודתית)
   
      * MLE: Maximum Likelihood Estimation (אמד ניראות מירבית)
      
      * Moments method (אמידה בשיטת המומנטים)
   
   * Bias and variance of point estimates
   
   * Confidence intervals (incl. examples)
   
---

# Parameters, estimators, and estimates

   * Example here: [https://sarid.shinyapps.io/population_vs_sample/](https://sarid.shinyapps.io/population_vs_sample/)

--

   * The difference between parameters, estimators, and estimates
   
      * $\theta$ is a *population parameter* (e.g., $\mu$) פרמטר
      
      * Estimated by the statistic $\hat{\Theta}$, a *point estimator* (e.g., $\bar{X}=\frac{1}{n}\sum{x_i}$) אמד נקודתי
      
      * When it is computed (from a sample), it is called a *point estimate* אומדן 
      
---

# How can we find a formula for point estimates?
   
   * Let's play a game.
   
   * If we were to randomly sample from a normal distribution, where would the observations fall?

```{r hidden parameters, include=FALSE}
density_func <- tibble(x = seq(-2, 2, 0.1)) %>% 
   mutate(dens = dnorm(x = x, mean = 0, sd = 1))

set.seed(0)
randomized_dots <- tibble(rnd = rnorm(n = 1000, mean = 0.4, sd = 1.1))

```

```{r, eval=TRUE, echo=FALSE, fig.dim=c(5, 3)}

ggplot(density_func, aes(x = x, y = dens)) + 
   geom_line()

```

   * File `01-Estimation-leading_up_to_MLE.R`.
   
---

# Maximum Likelihood Estimation (MLE) אמד ניראות מירבית

An important and very common approach to solving estimation problems in statistics. The idea is as follows:

If you want to estimate some population parameter $\theta$, use the most **likely** value. 

--

Let $f(x;\theta)$ represent the density function of $X$. Given a sample $x_1,\dots,x_n$ in which $x_i$'s are independent, we can write the likelihood (ניראות) of the sample:

$$L(\theta)=f(x_1,\ldots,x_n;\theta)=f(x_1;\theta)\times\ldots\times f(x_n;\theta)=\prod_{i=1}^nf(x_i;\theta)$$

--

   * The product (multiplication) can be used thanks to the assumption of independence of the sampled observations.
   
   * $L(\theta)$ is a function of $\theta$ (only) once the sample has been set.

   * In the discrete case we will use $P(X_1 = x_1,\ldots,X_n=x_n;\theta)$. The likelihood is the same as the probability of obtaining the sample.

---

# MLE example - Poisson distribution (התפלגות פואסון)

The Poisson distribution is used to represent a counting processes. I.e., the number of accumulated events is distributed Poisson.

--

$$P(X=x)=\frac{\lambda^xe^{-\lambda}}{x!}$$

--

```{r demonstration poisson, fig.dim=c(7, 3), echo=FALSE}

poiss_demonstration <- crossing(x = 0:25, pois_rate = c(0.5, 1, 2, 10)) %>% 
   mutate(dpois = map2_dbl(x, pois_rate, dpois))

ggplot(poiss_demonstration, aes(x = x, y = dpois, fill = factor(pois_rate))) +
   geom_col() + 
   facet_wrap(~pois_rate) + 
   theme_bw()

```

---

# MLE example - Poisson - cont.

.medium[
Assume we sample the number of arrivals to the hospital during $n$ weekdays: $x_1,\ldots,x_n$. The number of arrivals is distributed with a $\text{Poisson}(\lambda)$ distribution.]

--

.medium[Then, the likelihood is:

$$L(\lambda) = \prod_{i=1}^n{(\lambda^{x_i}e^{-\lambda})/x_i!}$$
]

--

.medium[Taking logarithm we have:

$$\log L(\lambda)=\sum_{i=1}^n{\left(x_i\log\lambda-\lambda\log(e)-\log(x_i!)\right)}$$
]

--

.medium[
Now we require an extremum, i.e. $d\log L(\lambda)/d\lambda=0$:

$$\sum_{i=1}^n{x_i/\lambda^*}-n=0 \Longrightarrow \lambda^*=\bar{X}$$
]

--

.medium[
This is indeed a maximum $\frac{d^2\log{L(\lambda)}}{d\lambda^2}=-\left(\sum_{i=1}^n{\frac{x_i}{\lambda}}\right)<0$
]

---

# MLE example - Bernulli distribution

Let's assume we conducted $n$ experiments each with probability $p$ for success and $q=1-p$ for failure. These are Bernoulli i.i.d variables.

$$B_i=\left\{\begin{array}{ll}1 & \text{w.p. } p\\ 0 & \text{w.p. } 1-p\end{array}\right.$$

Assume we got $v$ success, $n-v$ failures. 

   * What is the Liklihood?
   * What is the log-Liklihood?
   * Find the optimal $\hat{p}$.

---

# Maximum Liklihood Estimate - Normal distribution

.medium[The previous examples dealt with discrete distributions. What happens in the continuous case?]

--

.medium[
Assume $X_1,\ldots,X_n\sim \mathcal{N}(\mu,\sigma)$ i.i.d distributed. We sample $x_1,\ldots,x_n$ and we are looking for an MLE for $\mu$ and $\sigma^2$:
]

--

.medium[The likelihood:

$$L(\mu)=\prod_{i=1}^n\left(\frac{e^{\frac{-(x_i - \mu)^2}{2 \sigma^2}}}{\sqrt{2\pi}\sigma}\right)=\frac{1}{(2\pi\sigma^2)^{n/2}}\exp{\left[-\frac{1}{2}\sum_{i=1}^n{\left(\frac{x_i-\mu}{\sigma}\right)^2}\right]}$$
]

--

.medium[Taking $\log$ we get:

$$\log L(\mu,\sigma^2)=-\frac{n}{2}\log(2\pi)-\frac{n}{2}\log(\sigma^2)-\frac{1}{2}\sum_{i=1}^n\left(\frac{x_i-\mu}{\sigma}\right)^2$$
]

--

.medium[
$$\frac{\partial\log L(\mu,\sigma^2)}{\partial \mu}=\sum_{i=1}^n{\frac{x_i-\mu}{\sigma}} \Longrightarrow \mu^*=\bar{X}$$
]

---

# Maximum Liklihood Estimate - Normal distribution

.medium[Now, derivative by $\sigma$ we obtain:

$$\frac{\partial\log L(\mu,\sigma^2)}{\partial (\sigma^2)}=-\frac{n}{2}\frac{1}{\sigma^2}+\frac{1}{2}\frac{1}{\sigma^4}\sum_{i=1}^n(x_i-\mu)^2$$

From here we get:

$$(\sigma^*)^2=\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2$$
]

<!-- As you have seen, $(\sigma^*)^2$ is a **biased** estimator to the variance (the unbiased estimator $s^2$ had $n-1$ in the denominator). -->

---

# Method of Moments אמידה בשיטת המומנטים

In this method, we use knowledge about the moments of the distribution (i.e. $E[X], E[X^2], E[X^3], \ldots$), express the parameters as functions of these moments, and then use the sample moments to compute our estimator.

Let $f(x)$ be a density function for $X_1,\ldots,X_n$, then:

| Moment | Continuous | Discrete | Sample |
|--------|------------|----------|--------|
| $E[X]$ | $\int{xf(x)dx}$ | $\sum_k{kP(X=k)}$ | $(1/n)\sum_{i=1}^{n}{x_i}$ |
| $E[X^2]$ | $\int{x^2f(x)dx}$ | $\sum_k{k^2P(X=k)}$ | $(1/n)\sum_{i=1}^{n}{x_i^2}$ |
| $E[X^m]$ | $\int{x^mf(x)dx}$ | $\sum_k{k^mP(X=k)}$ | $(1/n)\sum_{i=1}^{n}{x_i^m}$ |

The sample mean $\bar{X}=(1/n)\sum_{i=1}^n{x_i}$ is the moment estimator of the population mean.

---

# Method of Moments - Example - the Normal disribution

The first moment is $E[X]=\mu$, hence $\hat{\mu}=\bar{X}$ is the moment estimator for the population mean.

--

The second moment is $E[X^2] = \mu^2 + \sigma^2$ require:

$$\mu^2 + \sigma^2 = \frac{1}{n}\sum_{i=1}^n{x_i^2}$$

--

Set $\mu^2=\left(\frac{1}{n}\sum_{i=1}^n{x_i}\right)^2$

--

$$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^n{x_i^2} - \left(\frac{1}{n}\sum_{i=1}^n{x_i}\right)^2=\frac{\sum{x_i^2}-(\sum{x_i})^2/n}{n}=\frac{\sum_{i=1}^n(x_i-\bar{X})^2}{n}$$

<!-- Again, this is a biased estimator for $\sigma^2$, usually we will use $s^2$ seen earlier. -->

---

# An estimator is a **random variable** (אמד הוא משתנה מקרי!)

   * An estimator is the result of sampling

--

   * An estimator is a random variable

--
   
   * For example, $\bar{X}$, is a random variable.
   
   * What is $E[\bar{X}]$? what is $\operatorname{Var}(\bar{X})$?
   
--
   
      * $E[\bar{X}] = \mu, \operatorname{Var}(\bar{X}) = \frac{\sigma^2}{n}$
   
--
      
   * If $n$ is "large enough" (i.e., $n\geq30$) is also approximately normally distributed (central limit theorem משפט הגבול המרכזי)
      
--

   * We will use this property **A lot** during this course
   
---

# The Central Limit Theorem

If $\bar{X}$ is the mean of a random sample of size $n$ taken from a population with mean $\mu$ and finite variance $\sigma^2$, then the limiting form of the distribution of 

$$Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$$

As $n\rightarrow\infty$ is the standard normal distribution $N(0,1)$

For most purpuses, $n\geq30$ is considered "large enough" (as a rule-of-thumb). 

Lets look at an example of exponential distribution: $\operatorname{Exp}(\lambda=1)$. **What are $\mu$ and $\sigma$?**

---

# The distribution of $\bar{X}$ with varying $n$

With $\operatorname{Exp}(\lambda=1)$ we have $\mu=\sigma=1/\lambda=1$

Try increasing `sample_size` in the underlying code and see what happens to the chart.

```{r example how a sum of exp becomes normal, message=FALSE, warning=FALSE, echo=FALSE}
sample_size <- 1
lambda <- 1

rv_binom <- matrix(rexp(n = sample_size*1000, rate = lambda), nrow = sample_size, ncol = 1000) %>% 
  as_tibble(.name_repair = "unique") %>% 
  pivot_longer(cols = everything(), names_to = "var", values_to = "value") %>% 
  group_by(var) %>% 
  summarize(mean = mean(value)) %>% 
  mutate(standardized = (mean - 1/lambda)/((1/lambda)/sqrt(sample_size)))

ggplot(rv_binom, aes(x = standardized)) + 
  geom_histogram(bins = 100) + 
  theme_bw()

```

---

# Desired properties of point estimators
   
   * Unbiased: $E\hat{\Theta} - \theta = 0$ אמד חסר הטיה
   
      * In the case of $\bar{X}$ the bias is indeed 0, i.e.: $E\bar{X}-\mu=0$

--

   * Low variance: $\operatorname{Var}(\hat{\Theta})$ as low as possible אמד עם שונות נמוכה

--
   
I've shown that: 

   * The average $\bar{X}=\sum_{i=1}^n{x_i}$ is unbiased: 
      * $E[\bar{X}] = \mu$
      
   * Its variance is $V(\bar{X})=\frac{\sigma^2}{n}$
   
   * In fact, it is the *Minimum Variance Unbiased Estimate* (proof out of scope)
   
--
   
   * Question: let's say we sampled $n$ observation but only use $x_1$ as our estimator. 
   
      * What is its bias? 
      
      * What is its variance?

---

# Another example $S^2$ השונות המדגמית (אמד לשונות)
      
   * Sample variance: $S^2=\frac{\sum_{i=1}^n(x_i-\bar{X})^2}{n-1}$ is an **unbiased** estimator of $V(X)=\sigma^2$
   
      * $V(X)=E(X-E[X])^2=EX^2-(EX)^2$
   
   * Standard deviation: $S$ is a **biased** estimator for $\sigma$

---

# Why $S^2$ is an unbiased estimator to $\sigma^2$?

$S^2$ is unbiased:

$$ES^2=\frac{1}{n-1}E{\sum_{i=1}^n(x_i-\bar{X})^2}=\frac{1}{n-1}E{\sum_{i=1}^n(x_i^2+\bar{X}^2-2\bar{X}x_i)}=$$

--

$$=\frac{1}{n-1}E\left(\sum_{i=1}^nx_i^2-n\bar{X}^2\right)=\frac{1}{n-1}\left[\sum_{i=1}^n{Ex_i^2-nE(\bar{X}^2)}\right]$$

--

Using the fact that $E(x_i^2)=\mu^2+\sigma^2$ and that $E(\bar{X}^2)=\mu^2+\sigma^2/n$ we have:

--

$$E(S^2)=\frac{1}{n-1}\left[\sum_{i=1}^n(\mu^2+\sigma^2)-n(\mu^2+\sigma^2/n)\right]=\sigma^2$$

$$\square$$

---

# Why is $S$ a biased estimator to $\sigma$?

   * השונות המדגמית היא אמד חסר הטיה לשונות באוכלוסיה, אבל...
   
   * סטיית התקן המדגמית, אינה אמד חסר הטיה לסטיית התקן באוכלוסיה

We need to show that $ES\neq\sigma$. Let 

$$S=\sqrt{\sum_{i=1}^n{\frac{(x_i-\bar{X})^2}{n-1}}}$$

--

Consider that $0<\operatorname{Var}(S)=E[S^2]-(E[S])^2$ (this is true for any RV, specifically $S$ in this case)

--

Hence

$$(ES)^2<E[S^2] \Leftrightarrow ES<\sqrt{E[S^2]}=\sigma$$ 

$$\square$$

In certain cases, we can directly compute this bias (i.e. what is $ES-\sigma$), for example, if we assume $X\sim N(\mu,\sigma)$, then:

$$\sigma-E(S)\cong\frac{\sigma}{4n}$$ (see [here](https://stats.stackexchange.com/questions/11707/why-is-sample-standard-deviation-a-biased-estimator-of-sigma)).

---

# Statistical Intervals רווח בר סמך (אמידה מרווחית)

We discussed point estimates, however

   * Even if everything works "properly" (a random sample, unbiased estimator), it is unlikely that we will reach the exact parameter value
   
   * As the sample increases accuracy improves; but

--
   
   * Sometimes we are interested in a **Confidence Interval** רווח בר סמך
   
   * An interval of the form $\hat{\Theta}_l < \theta < \hat{\Theta}_u$ where 

--
   
   * The lower and upper bounds $\hat{\Theta}_l, \hat{\Theta}_u$ depend on the statistic $\hat{\Theta}$

--

In a probabilistic notation, we are looking for $\hat{\Theta}_l, \hat{\Theta}_u$ such that:

$$P(\hat{\Theta}_l < \theta < \hat{\Theta}_u) = 1-\alpha$$

For $\alpha\in(0,1)$. For example, when we set $\alpha=0.05$, we call this a 95% confidence interval for $\theta$.

רווח בר סמך של 95% (שגיאה של 5%)

---

# Sanity check

**What would be a 100% confidence interval**? 

   * I.e., what would be $\hat{\Theta}_l, \hat{\Theta}_u$ such that:

$$P(\hat{\Theta}_l < \theta < \hat{\Theta}_u) = 1$$

--

Setting $\hat{\Theta}_l=-\infty, \hat{\Theta}_u=\infty$ gives us a 100% confidence interval (i.e., the $\theta$ is a real number).

---

# Confidence Interval for Normal Distribution with Known Variance

We previously mentioned the central limit theorem and that 

$$Z = \frac{\bar{X}-\mu}{\sigma/\sqrt{n}}$$

Is normally distributed as $n\rightarrow\infty$. Hence:

$$P(z_{\alpha/2} < Z < z_{1-\alpha/2}) = 1-\alpha$$

--

$$P(z_{\alpha/2} < \frac{\bar{X}-\mu}{\sigma/\sqrt{n}} < z_{1-\alpha/2}) = 1-\alpha$$

--

Using the fact that for the normal distribution $z_{1-\alpha/2}=-z_{\alpha/2}$:

$$P(\bar{X} - z_{\alpha/2}\frac{\sigma}{\sqrt{n}} < \mu < \bar{X} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}) = 1-\alpha$$

---

# Example - determining the sample size from a desired confidence range

If we want to have a confidence interval with a range not exceeding $\pm r$, we can use:

$$\bar{X}+z_{\alpha/2}\frac{\sigma}{\sqrt{n}}-\left(\bar{X}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right) \leq 2r$$

--

Then,

$$\sqrt{n}\geq z_{\alpha/2}\frac{2\sigma}{2r}$$

--

$$n\geq \left(z_{\alpha/2}\frac{\sigma}{r}\right)^2$$

Ever wondered why surveys have $n=500$ respondents? it comes from a $\pm 4.4\%$ margin of error with a $95\%$ confidence interval (using a binomial distribution). 

---

# Confidence Interval for Normal Distribution with Unknown Variance

In this case, we use our estimator $S$ to compute our statistic and confidence interval.

$$T = \frac{\bar{X}-\mu}{S/\sqrt{n}}$$

The statistic $T$ has a student's t-distribution with $n-1$ degrees of freedom. I.e.:

$$P(-t_{\alpha/2,n} < T < t_{\alpha/2,n})=1-\alpha$$

.center[
```{r example for the t distribution, echo = F, fig.dim=c(7,4)}

t_dist_example <- tibble(x = -seq(-10, 10, 0.1)) %>% 
   mutate(t01 = dt(x, df = 1),
          t05 = dt(x, df = 5),
          t10 = dt(x, df = 10),
          norm = dnorm(x, mean = 0, sd = 1)) %>% 
   pivot_longer(cols = -x, names_to = "distribution", values_to = "y")

ggplot(t_dist_example, aes(x, y, color = distribution, linetype = distribution)) + 
   geom_line(size = 0.8) + 
   theme_bw()

```
]

---

# Visualizations

Show in `R`, if time permits.

---

# Summary

   * Population versus sample
   
   * Point estimates

   * Estimation using maximum likelihood
   
   * Estimation using the moments method
   
   * Bias and variance of point estimates
   
   * Confidence intervals (incl. examples)
   
Next week we will continue our discussion of confidence intervals and see the implementation in R.
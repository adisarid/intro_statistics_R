---
title: "Multiple Linear Regression: Intervals, Multicolinearity, Interactions, Stepwise Selection"
subtitle: ""
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}

.remark-code {
  font-size: 24px;
}

.huge { 
  font-size: 200%;
}
.tiny .remark-code {
  font-size: 50%;
}

.small .remark-code{
   font-size: 85% !important;
}

.small {
   font-size: 85% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

.small-slide {
   font-size: 70% !important;
}

.image-50 img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.right-plot {
   width: 60%;
   float: right;
   padding-left: 1%;
   bottom: 0px;
   right: 0px;
   position: absolute;
}



```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
library(countdown)
```

# Reminder from Previous Lecture

We focused on multiple linear regression, which assumes:

$$Y=\beta_0 + \beta_1X_1 + \beta_2X_2 + \ldots + \beta_pX_p + \epsilon$$

--
   
   * $\epsilon \sim \mathcal{N}(0,\sigma_\epsilon)$
   
   * For hypothesis testing on $\beta$ we also require homoscedastity

--

   * We used the relationship $SS_T = SS_R + SS_E$ to define:
   
   * The coefficient of determination $R^2 = \frac{SS_R}{SS_T}$

--

   * When $0\leq R^2\leq1$. High values correspond to a strong relationship
   
   * For the simple linear regression, we saw that $R^2=\hat\rho^2$, the correlation of $X$ and $Y$

---

<!-- # Reminder from Previous Lecture (2) - Solution of the Multiple Linear Regression -->

<!-- We represented the multiple linear regression as a matrix equation: -->

<!-- $$y = X\beta+\epsilon$$ -->

<!-- Where: -->

<!-- $$y = \left[\begin{array}{c}y_1\\\vdots\\y_n\end{array}\right], \quad X=\left[\begin{array}{cccc}1 & x_{11} & \ldots & x_{1k}\\\vdots & \vdots & \ddots & \vdots\\1 & x_{n1} & \ldots & x_{nk}\end{array}\right], \quad \beta = \left[\begin{array}{c}\beta_0\\\vdots\\\beta_k\end{array}\right],\quad \epsilon=\left[\begin{array}{c}\epsilon_1\\\vdots\\\epsilon_n\end{array}\right]$$ -->

<!-- -- -->

<!-- We are looking for $\beta$ which minimizes $L = \epsilon^t\epsilon=(y-X\beta)(y-X\beta)$ -->

<!-- $$\frac{\partial L}{\partial\beta}=0$$ -->

<!-- --- -->

<!-- # Reminder (3) - Least Squares Estimation of the Parameters -->

<!-- The resulting equations are given by: -->

<!-- $$X^tX\hat\beta=X^ty$$ -->

<!-- -- -->

<!-- In case that $X^tX$ is a non-singular matrix (i.e., invertible), the solution is unique and equals -->

<!-- $$\hat\beta = (X^tX)^{-1}X^ty$$ -->

<!-- -- -->

<!-- Once $\hat{\beta}$ is found, we can use it to predict our values: -->

<!-- $$\hat{y} = X\hat{\beta}$$ -->

<!-- -- -->

<!-- We can also compute the residuals: -->

<!-- $$e = y-\hat{y}$$ -->

<!-- -- -->

<!-- Let $p=k+1$ (the number of parameters including the constant $\beta_0$), then: -->

<!-- $$\hat\sigma^2=\frac{\sum{e_i^2}}{n-p}=\frac{SS_E}{n-p}$$ -->

<!-- Is an unbiased estimate of $\sigma_\epsilon^2$ -->

<!-- --- -->

# Reminder (2) - Hypothesis Tests on Individual Coefficients

We used the fact that:

   * $\hat\beta$ is unbiased and 
   
   * $\operatorname{Var}(\hat\beta_j)=\sigma^2[(X^tX)^{-1}]_{jj}$
   
To devise a hypothesis test for the individual $\hat\beta_j$:

   * $H_0: \beta_j=0$
   
   * $H_1: \beta_j\neq0$

With the statistic

$$T_0=\frac{\hat\beta_j}{\sqrt{\hat\sigma^2C_{jj}}}=\frac{\hat\beta_j}{\operatorname{se}(\hat\beta_j)}$$

---

# Reminder (3) - Hypothesis Tests on All Coefficients

We used the F-statistic 

$$F_0=\frac{SS_R/k}{SS_E/(n-k-1)}=\frac{MS_R}{MS_E}$$

To devise a broader test:

   * $H_0:\beta_1=\beta_2=\ldots=\beta_k=0$
   
   * $H_1: \exists i \text{ such that } \beta_i\neq0$

--

.small[We examined the corresponding ANOVA table]

.small[
| Source of Variation | Sum of Squares | df | Mean Squares | $F_0$ |
|----------------------|-----------------|:-----:|:------------:|---------------------|
| Regression | $SS_R$ | $k$ | $MS_R$ | $\frac{MS_R}{MS_E}$ |
| Error | $SS_E$ | $n-k-1$ | $MS_E$ |  |
| Total | $SS_T$ | $n-1$ |  |  |
]

---

# The adjusted $R^2$

We defined the adjusted $R_\text{adj}^2$: 

$$R^2_\text{adj}= 1 - \frac{SS_E/(n-p-1)}{SS_T/(n-1)}$$

Which provides a penalty for increasing the number of parameters, however it does not necessarily help us avoid over-fitting.

---

# Today

   * Confidence and prediction intervals
   
   * Multicolinearity
   
   * Interactions
   
   * Stepwise selection

---

# Mean Response Confidence Interval and Prediction intervals

The point estimate for a new response at a point $x_0=[1, x_{01}, \dots, x_{0k}]^t$ is:

$$\hat\mu_{Y|x_0} = x_0^t\hat\beta=\beta_0 + \beta_1x_1+\ldots+\beta_px_p$$

--

The estimator is unbiased and its variance is:

$$\operatorname{Var}(\hat\mu_{Y|x_0}) = \sigma^2x_0^t(X^tX)^{-1}x_0$$

--

Hence, we can use the following statistic for building confidence and prediction intervals:

$$\frac{\hat\mu_{Y|x_0} - \mu_{Y|x_0}}{\sqrt{\hat\sigma^2x_0^t(X^tX)^{-1}x_0}}$$

--

Let's see this in action and discuss it with the following R Shiny app:
[https://sarid.shinyapps.io/intervals_demo/](https://sarid.shinyapps.io/intervals_demo/)

--

.small[
Confidence and prediction intervals:

$$\hat\mu_{Y|x_0} + t_{\alpha/2, n-p}\sqrt{\hat\sigma^2x_0^t(X^tX)^{-1}x_0} \leq \mu_{Y|x_0} \leq \hat\mu_{Y|x_0} + t_{1-\alpha/2, n-p}\sqrt{\hat\sigma^2x_0^t(X^tX)^{-1}x_0}$$

$$\hat y_0 + t_{\alpha/2, n-p}\sqrt{\hat\sigma^2(1+x_0^t(X^tX)^{-1}x_0)} \leq Y_0 \leq \hat y_0 + t_{1-\alpha/2, n-p}\sqrt{\hat\sigma^2(1+x_0^t(X^tX)^{-1}x_0)}$$
]

---

# Questions about confidence and prediction intervals

   * Note that confidence interval relates to the linear relationship itself.
   
   * Prediction interval relates to future observations.

--

In groups, use the confidence and prediction intervals simulation, and answer the following questions:

   * The *"define error sigma"* controls $\sigma_{\epsilon}$. Try moving it to 0 and start increasing it. Follow the $y$ axis closely while doing so. What happens and why?

   * What happens to the confidence interval (dashed blue lines) and the prediction interval as the sample size increases?
   
   * What happens to the brown line (the model) when the sample size increases?
   
   * Add the points to the chart (click on *"Points"* under *"include on chart"*). How many points would you expect to find within the red lines? How many within the blue lines?
   
   * Try the Sine relationship (under *Choose type of relationship*). How would you run `lm` to build such a model in `R`?
   
   * When showing the Sine relationship, try increasing the error (increase in *define error sigma*). What happens to the relationship? why?


```{r countdown regression experimentation}
countdown::countdown(minutes = 10L, top = "10px")
```

---

# Multicolinearity

aaa

---

# Interactions

Using the `palmerpenguins` data:

   * Three types of penguins.
   * Describe the relationship between bill length and bill depth.

.small[
```{r interaction example, message=FALSE, warning=FALSE, fig.dim=c(6, 4)}
library(palmerpenguins)
ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm)) + 
   geom_point() + 
   stat_smooth(method = "lm")
```
]

---

# Interactions

This time, considering the species.

.small[
```{r with interaction of species, message=FALSE, warning=FALSE, fig.dim=c(6, 4)}
ggplot(penguins, aes(x = bill_depth_mm, y = bill_length_mm, color = species)) + 
   geom_point() + 
   stat_smooth(method = "lm")
```
]

The relationship is opposite! How can we solve this?

--

We can either build three models (one for each species like in `ggplot2`) or use interaction in the `formula` argument.


---

# The models with and without interaction

Set $l=$length, $d=$depth.

Without interaction:

$$l=\beta_0 + \beta_1 \times d + \epsilon$$

With interaction:

$$l=\beta_0 + \beta_1 \times d +
\beta_2 \times d \times \operatorname{Chinstrap} + 
\beta_3 \times d \times \operatorname{Gentoo} +
\epsilon$$

In action:

.tiny[
```{r example}

lm(formula = bill_length_mm ~ bill_depth_mm*species, data = penguins) %>% 
   summary()

```
]

---
 
# Variable Selection via Stepwise Regression

In cases we have many independent variables (i.e., $X_i$s) we want to reduce the complexity of the regression model, and focus only on the most important or most influential variables.

--

For that, we can use the **stepwise regression** algorithm.

   * Backward elimination
   
   * Forward selection
   
   * Backward-forward

--

All algorithms are greedy algorithms: look one step ahead, and choose the best course of action. Hence, the might not reach the optimal solution.

---

# The Backward Elimination Algorithm

   1. Set current model = the "full" model (all independent variables)

--
   
   2. Use some accuracy measure to examine the influence of the removal of each of the variables. E.g., the Akaike Information Criteria (AIC = $-2\log(L) + 2p$)

--
   
   3. Is there a variable that its omission improves our measure? (e.g. decrease of the AIC)? 
   
      a. **Yes**: Remove the variable which its removal contributes the most (to the improvement of the measure), Update the current model accordingly, and go to step 2. 
      
      b. **No**: Algorithm stops and outputs the current model.

--
      
## The forward selection

A similar algorithm, instead of removing, add variables one at a time

--

## The backward-forward

Combination, at each iteration check both removal and addition.

---

# Intuition for the AIC

AIC = $-2\log(L) + 2p$

The AIC uses the likelihood $L$:

$$L = \prod_{i=1}^n{f(\hat{y}_i)}$$

(Higher likelihood value is better, hence lower $-2\log(L)$ is better)

--

As the number of parameters in the model increases, the model is more prone to overfitting, hence we apply a penalty of $2p$ (a high $p$, i.e., a lot of parameters is bad for us).

--

Hence, we want to minimize the AIC.

The value of AIC has no meaning, except for the ability to compare two models.

--

Other measures also exist, e.g.: 

   * Montgomery uses the F-Statistic
   
   * Another measure is the BIC = $- 2\log(L) + \log(n)p$

---

# Stepwise Regression - Example: Car Efficiency.

.tiny[
```{r mtcars stepwise regression example}
mtcars_lm <- lm(formula = mpg ~ ., data = mtcars)
step(mtcars_lm, direction = "backward")
# step(lm(formula = mpg ~ 1, data = mtcars), 
#      direction = "forward",
#      scope = formula(lm(mpg ~ ., data = mtcars)))
```
]

---

# Refresher and Recap - Test-like Exercises: Descriptive Statistics

Now we are going to solve four exercises which might resemble exercises which will appear in the test.

.small[Explain the different elements of a boxplot, i.e., in the following chart: 

   1. What does the line in the middle of the box (the blue area) stands for?
   
   2. What do the top and bottom boundaries of the chart's box stand for?
   
   3. What are the whiskers and how do they help us?
   
   4. What does the dot at the upper side of the chart stands for?
   
   5. How would you use a boxplot to recognize a normal distribution?
   
```{r boxplot question, echo=FALSE}
ggplot(mtcars, aes(y = mpg, x = "")) + 
   geom_boxplot(fill = "lightblue") + 
   xlab("") +
   theme_bw()
```
]

---

# Test-like Exercises: Confidence and Prediction Intervals

The following exercise is similar to Walpole, *et al.* (Chapter 9, page 245, ex. 7).

A random sample of $n=100$ car owners show that in the state of Virginia, a car drives 23,500 km per year with a sample standard deviation of 3900 km.

   a. Construct a 99% confidence interval for the average number of kilometers a car is driven annually in Virginia.
   
   b. What can we assert with a 99% confidence interval about the possible size of our error if we estimate the average number of kilometers driven by car owners in Virgina to be 23,500 km?
   
   c. Danny has just moved to Virginia, provide an upper bound (one sided prediction interval) at a 95% confidence interval to the amount of kilometers Danny will drive in the next year.
   
---

# Test-like Exercises: Two Sample Tests, p-value

The following exercise is from Walpole, *et al.*, (chapter 9, page 319, ex. 9).

A study at the University of Colorado shows that running increases the percent resting metabolic rate (RMR) in older women. The average RMR of 30 elderly women runners was 34.0% higher than the average RMR of 30 sedentary elderly women. The standard deviations were 10.5 and 10.2 respectively. 

Was there a significant increase in RMR of the women runnders over the sedentary women?

Assume the populations are normally distributed with equal variances. Use a p-value to report you conclusions.

---

# Test-like Exercises: Interpretation of a Model's Output

.small[
The `ChickWeight` dataset contains the results of a feeding experiment of 50 chicks' (`Chick`) with their tracked weight (`weight`), over a period of 21 days (`Time`), each chick was subjected to a different type of diet (`Diet`).

In the following model (see bottom of slide), we are using the interaction of `Time*factor(Diet)` as one of the explanatory variables, along with `Time` as another explanatory variable. The dependent variable is the chick's `weight`.

Questions:

   1. The original `Diet` variable is numeric. Why are we using it in the regression model as `factor(Diet)`?

   1. How many levels does the `factor(Diet)` variable has, explain.
   
   2. Why do we need the interaction of `Time*factor(Diet)` in the model? (why is `weight ~ Time + factor(Diet)` not enough)
   
   3. Which dietary method helps increase the chick's weight the most? Explain how you deduced this from the model's output.

See the next slide for an additional question.
]

.tiny[
```{r output exercise}
chick_lm <- lm(formula = weight ~ Time + Time*factor(Diet), data = ChickWeight)
summary(chick_lm)
```
]

---

# Test-like Exercises: Interpretation of a Model's Output - continued

Look at the following qqplot of residuals and plot of the residuals as a function of time. 

   4. Explain what are the underlying assumptions of the linear regression model.
   
   5. Observing the residuals' plots below, would you say that any of the linear regressions assumptions are violated? which one?

```{r qqplot and residuals, echo=FALSE, fig.dim=c(10, 5)}
chick_residuals <- as_tibble(ChickWeight) %>% 
   mutate(resid = chick_lm$residuals)
chick_qqplot <- ggplot(chick_residuals, aes(sample = (resid - mean(resid))/sd(resid))) + 
   geom_qq() + 
   theme_bw() + 
   ggtitle("QQ plot of residuals")
chick_residuals_plot <- ggplot(chick_residuals, aes(x = Time, y = resid)) + 
   geom_jitter() +
   facet_wrap(~{paste0("Diet=", Diet)}) + 
   theme_bw() + 
   ggtitle("Residuals as a function of time")
cowplot::plot_grid(chick_qqplot, chick_residuals_plot)
```
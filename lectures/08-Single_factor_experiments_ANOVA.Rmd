---
title: "Analysis of Single-Factor Experiments (ANOVA)"
subtitle: "Lecture #10"
author: "Adi Sarid"
institute: "Tel-Aviv University"
date: "updated: `r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
    css: [metropolis, rutgers-fonts]
---

```{css, echo = FALSE}

.remark-code {
  font-size: 24px;
}

.huge { 
  font-size: 200%;
}
.tiny .remark-code {
  font-size: 50%;
}

.small .remark-code{
   font-size: 85% !important;
}

.small {
   font-size: 85% !important;
}

.remark-slide-content {
    font-size: 20px;
    padding: 1em 4em 1em 4em;
}

table { display: inline-block; }

th, td {
   padding: 5px;
}

.small-slide {
   font-size: 70% !important;
}

.image-50 img {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 50%;
}

.right-plot {
   width: 60%;
   float: right;
   padding-left: 1%;
   bottom: 0px;
   right: 0px;
   position: absolute;
}



```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE, fig.width = 3, fig.height = 3)
knitr::opts_chunk$set(fig.dim=c(3, 3), fig.align = "center")
library(tidyverse)
library(countdown)
```

# Reminder from Previous Lecture (Multiple Regression Exercise)

.small[
The `ChickWeight` dataset contains the results of a feeding experiment of 50 chicks' (`Chick`) with their tracked weight (`weight`), over a period of 21 days (`Time`), each chick was subjected to a different type of diet (`Diet`).

In the following model (see bottom of slide), we are using the interaction of `Time*factor(Diet)` as one of the explanatory variables, along with `Time` as another explanatory variable. The dependent variable is the chick's `weight`.

Questions:

   1. The original `Diet` variable is numeric. Why are we using it in the regression model as `factor(Diet)`?

   1. How many levels does the `factor(Diet)` variable has, explain.
   
   2. Why do we need the interaction of `Time*factor(Diet)` in the model? (why is `weight ~ Time + factor(Diet)` not enough)
   
   3. Which dietary method helps increase the chick's weight the most? Explain how you deduced this from the model's output.

See the next slide for an additional question.
]

.tiny[
```{r output exercise}
chick_lm <- lm(formula = weight ~ Time + Time*factor(Diet), data = ChickWeight)
summary(chick_lm)
```
]

---

# Test-like Exercises: Interpretation of a Model's Output - continued

Look at the following qqplot of residuals and plot of the residuals as a function of time. 

   4. Explain what are the underlying assumptions of the linear regression model.
   
   5. Observing the residuals' plots below, would you say that any of the linear regressions assumptions are violated? which one?

```{r qqplot and residuals, echo=FALSE, fig.dim=c(10, 5)}
chick_residuals <- as_tibble(ChickWeight) %>% 
   mutate(resid = chick_lm$residuals)
chick_qqplot <- ggplot(chick_residuals, aes(sample = (resid - mean(resid))/sd(resid))) + 
   geom_qq() + 
   theme_bw() + 
   ggtitle("QQ plot of residuals")
chick_residuals_plot <- ggplot(chick_residuals, aes(x = Time, y = resid)) + 
   geom_jitter() +
   facet_wrap(~{paste0("Diet=", Diet)}) + 
   theme_bw() + 
   ggtitle("Residuals as a function of time")
cowplot::plot_grid(chick_qqplot, chick_residuals_plot)
```

---

# Experiment Design - Motivation

With methods of hypothesis testing, we were able to discern the differences between two groups (i.e., unpaird two-sample test).

For example trying to see if cars with 4 cylinders are more efficient than cars with 8 cylinders.

.small[
```{r mtcars compare cyls}
mtcars %>% 
   filter(cyl != 6) %>% 
   t.test(formula = mpg ~ cyl, data = .)
```
]

---

# Experiment Design - Motivation (2)

Sometimes, we have more than two levels, in which case, we would like to devise a test which will compare all levels.

--

In the case of a single variable with multiple levels, this is called *single factor experiments* (in the next lecture we will also discuss *multi factor experiments*, when there are multiple levels).

--

The process invloves:

   * Conjecture - The hypothesis that motivates the experiment
   
   * Experiment - The actual test performed to investigate the conjecture
   
   * Analysis - Statistical analysis of the collected data
   
   * Conclusions - What has been learned
   
The process is iterated: to improve the experiment (e.g., add new variables or change the methods) and learn more.

--

.small[The following material is covered in Montgomery chapter 13.]

---

# Experiment Design - Motivation (3)

We would like a statistical that would highlight the efficiency of cars as a function of the number of cylinders. 

--

A boxplot can visually illustrate what we are looking for, but we cannot yield strength or significance from it.

```{r mtcars cyl comparison}
ggplot(mtcars, aes(y = mpg, x = factor(cyl))) + 
   geom_boxplot(fill = "lightblue") + 
   theme_bw()
```

--

Question: How would you use linear regression to examine this relationship?

---

# The Completely Randomized Single-Factor Experiment

Each factor level is called a *treatment* (i.e., for different treatments administered in an experiment).

--

The experimenter randomely samples subjects (either of equally sized groups or varying sized groups).

--

We describe the observations for a **completely randomized design** by a linear statistical model:

$$Y_{ij} = \mu + \tau_i + \epsilon_{ij}, \quad i = 1,\ldots,\ j=1,\ldots n$$

The value $\mu_i=\mu+\tau_i$ is the mean value of the $i$th treatment.

--

We assume that the errors $\epsilon_{i,j}$ are normally and independently distributed $\mathcal{N}(0, \sigma^2)$.

--

Two methods to select the treatments:

   * **Fixed-effects model**: the $a$ treatments were chosen specifically.
   
   * **Random-effects model** (components of variance): the $a$ treatments were a random sample from a larger population of treatments. We would like to extend the conclusions for additional treatments.

--

For now, we are going to focus on the **fixed effects model**.

---

# The Fixed-Effects Model

The treatments $\tau_i$ are defined as deviations from the overall mean $\mu$ such that: 

$$\sum_{i=1}^a{\tau_i}=0$$

--

Define:

$$y_{i\cdot}=\sum_{j=1}^n{y_{ij}}, \quad \bar{y}_{i\cdot}=y_{i\cdot}/n$$

$$y_{\cdot\cdot} = \sum_{i=1}^a\sum_{j=1}^n{y_{ij}}, \quad \bar{y}_{\cdot\cdot}=y_{\cdot\cdot}/N, \quad N=a\cdot n$$

--

We are interested in the following test:

   * $H_0: \tau_1=\ldots=\tau_a=0$
   
   * $H_1: \exists i\ |\ \tau_i\neq0$

---

# The Sum of Squares

Again, we use the sum of squares equation: $SS_T = SS_\text{Treatments} + SS_E$:

$$\sum_{i=1}^a\sum_{j=1}^n({y_{ij} - \bar{y}_{\cdot\cdot})^2} = n\sum_{i=1}^a{(\bar{y}_{i\cdot} - \bar{y}_{\cdot\cdot})^2} + \sum_{i=1}^a\sum_{j=1}^n({y_{ij} - \bar{y}_{i\cdot})^2}$$

--

The expected value of each of the errors is given by:

$$E(SS_\text{Treatments})=(a-1)\sigma^2 + n\sum_{i=1}^a{\tau_i^2}$$

$$E(SS_E) = a(n-1)\sigma^2$$

--

The degrees of freedom of each error are: 

   * $an-1$ for $SS_T$, 
   * $a-1$ for $SS_\text{Treatments}$ 
   * $a(n-1)$ for $SS_E$
   
---

# The ANOVA Test

If the null hypothesis holds, then $\tau_1=\ldots=\tau_a=0$, and

--

$MS_\text{Treatments}=SS_\text{Treatments}/(a-1)$ is an unbiased estimator of $\sigma^2$

--

The error mean square $MS_E=SS_E/[a(n-1)]$ is also an unbiased estimator of $\sigma^2$

--

Hence the following statistic has an F-distribution

$$F_0=\frac{SS_\text{Treatments}/(a-1)}{SS_E/[a(n-1)]}=\frac{MS_\text{Treatments}}{MS_E}$$

With $a-1$ and $a(n-1)$ degrees of freedom.

--

We use an upper-tail, one-sided critical region and reject $H_0$ if $f_0>f_{\alpha, a-1, a(n-1)}$.

---

# The ANOVA Table

The corresponding ANOVA table, for a Single-Factor Experiment, with a Fixed-Effects Model:

| Source of Variation | Sum of Squares | df | Mean Squares | $F_0$ |
|----------------------|-----------------|-------|:------------:|---------------------|
| Treatments | $SS_\text{Treatments}$ | $a-1$ | $MS_\text{Treatments}$ | $\frac{MS_\text{Treatments}}{MS_E}$ |
| Error | $SS_E$ | $a(n-1)$ | $MS_E$ |  |
| Total | $SS_T$ | $an-1$ |  |  |

--

For an unbalanced experiment (each treatment has varying group size, $n_i$) we would have:

$$SS_T=\sum_{i=1}^a\sum_{j=1}^{n_i}{y_{ij}^2} - \frac{y_{\cdot\cdot}^2}{N}$$

$$SS_\text{Treatments} = \sum_{i=1}^a\frac{y_{i\cdot}^2}{n_i} - \frac{y_{\cdot\cdot}^2}{N}$$

And $SS_E = SS_T-SS_\text{Treatment}$

---

# ANOVA Example

```{r anova example in mtcars}
mtcars2 <- mtcars %>% 
   mutate(cyl_fct = factor(cyl))
mtcars_anova <- aov(formula = mpg ~ cyl_fct, data = mtcars2)
summary(mtcars_anova)
```

--

Usually, ANOVA will be followed by a multiple comparisons procedures, that will help identify which factors contribute to the variation.

--

There are many such multiple comparison tests. See [this book](http://www.ievbras.ru/ecostat/Kiril/R/Biblio_N/R_Eng/Bretz2011.pdf): Bretz F., Hothorn T., and Westfall P., Multiple Comparisons Using R, CRC Press, 2011.

---

# Dunnett's  Test

The one-sided Dunnett's test takes the minimum (or maximum depending on direction) of the $a$ pairwise $t$ tests:

$$t_i=\frac{\bar{y}_{i\cdot}-\bar{y}_0}{s\sqrt{\frac{1}{n_i} + \frac{1}{n_0}}}$$

Where $y_0$ relates to the control group (to which we are comparing the treatments), and $s^2$ is the pooled variance, i.e.:

$$s^2=\sum_{i=0}^m\sum_{j=1}^{n_j}(y_{ij}-\bar{y}_{i\cdot})^2/v$$

With $v = \sum_{i=0}^m{n_i}-(a+1)$ degrees of freedom.

--

The Dunnett's test is availble in `multcomp` like many other procedures for multiple comparisons.

---

# Dunnett's Test Example
.small[
```{r Dunnetts test example}
suppressWarnings(suppressMessages(library(multcomp)))
glht(mtcars_anova,
     linfct = mcp(cyl_fct = "Dunnett"),
     alternative = "less") %>% 
   summary()
```
]

---

# Verifying ANOVA assumptions
.small[
The ANOVA model assumes that observations are normally and independently distributed, with the same variance for each treatment. 

This can be verified using hypothesis tests on the residuals, or viewing a proper plot (e.g., qqplot). The following illustrates that some assumptions are invalid in the previous example (which?) 
]
.tiny[
```{r verifying normality, fig.dim=c(8, 3.5)}
mtcars2_resid <- mtcars2 %>% 
   mutate(resid = mtcars_anova$residuals)
boxplot_resid <- ggplot(mtcars2_resid, aes(x = cyl_fct, y = resid)) + 
   geom_boxplot(fill = "lightblue") +
   theme_bw() + ggtitle("Boxplot of residuals in each cyl level")
qqplot_resid <- ggplot(mtcars2_resid, aes(sample = (resid - mean(resid))/sd(resid))) + 
   geom_qq() + 
   theme_bw() + ggtitle("QQ plot of residuals")
cowplot::plot_grid(qqplot_resid, boxplot_resid)
```
]

---

# Determining the Sample Size

The selection of the sample size is based on the difference we want to detect, and at what test power:

$$1-\beta = P(\text{Reject } H_0 | H_1) = P(F_0>f_{1-\alpha, a-1, a(n-1)} | H_1)$$

--

The effect size represents the differences in means between groups:

$$\operatorname{ES} = \frac{\mu_\text{experiment}-\mu_\text{control}}{s}$$
.small[
```{r determining anova sample size}
pwr::pwr.anova.test(k = 3, n = NULL, f = 0.25, sig.level = 0.05, power = 0.8)
```
]

---

# The Random-Effects Model (Factor with Many Levels)

In cases the factor has a large number of levels, we cannot sample all of them. We would like to sample part of them, $a$ levels, and draw conclusions about the entire population of factor levels.

--

As before, we define

$$Y_{ij} = \mu + \tau_i + \epsilon_{ij}$$

However, this time, the factor has a variance, i.e.:

$$\operatorname{Var}(Y_{ij}) = \sigma_\tau^2 + \sigma^2$$

--

We would like to test the hypothesis that:

   * $H_0:\sigma_\tau^2=0$
   
   * $H_1:\sigma_\tau^2>0$
   
If $H_0$ holds, this means that the treatments are identical. 

--

If $\sigma_\tau^2>0$ this means that there is variability between treatments.

---

# The Random-Effects Model - Continued

The relationship of residuals still holds:

$$SS_T = SS_\text{Treatments} + SS_E$$

But the errors change:

$$E(MS_\text{Treatments}) = E\left[\frac{SS_\text{Treatments}}{a-1}\right] = \sigma^2 + n\sigma^2_\tau$$

$$E(MS_E) = E\left[\frac{SS_E}{a(n-1)}\right] = \sigma^2$$

--

Under the null hypothesis both sizes are identical and hence

$$F_0=\frac{MS_\text{Treatments}}{MS_E}$$

Is an F-distributed variable with $a-1$ and $a(n-1)$ degrees of freedom.

---

# Determining the Sample Size in the Random Effects Model

We would like to find $n$ for a given $\beta$ (type-II error) such that:

$$1-\beta = P(\text{Reject } H_0|H_1)=P(F_0>f_{\alpha, a-1, a(n-1)}|\sigma_\tau>0)$$

--

If $H_1$ is true, then the ratio $\frac{MS_\text{Treatments}/(\sigma^2+n\sigma_\tau^2)}{MS_E/\sigma^2}$ has the $F$-distribution with $a-1$ and $a(n-1)$ degrees of freedom. Then,

$$1-\beta = P\left(\frac{MS_\text{Treatements}}{MS_E}>f_{\alpha, a-1, a(n-1)}|\sigma^2_\tau>0\right) = 
P\left(\frac{MS_\text{Treatements}}{MS_E/(1+n\sigma^2_\tau/\sigma^2)}>\frac{f_{\alpha, a-1, a(n-1)}}{(1+n\sigma^2_\tau/\sigma^2)}\right)$$

--

Hence,

$$1-\beta = P\left(F_{a-1, a(n-1)}>\frac{f_{\alpha, a-1, a(n-1)}}{(1+n\sigma^2_\tau/\sigma^2)}\right)$$

If we assume something about $\sigma_\tau^2/\sigma^2$, we can calculate the desired $n$, using the $F$ distribution function.

---

# Random-Effects Model - Example

When manufacturing food and drugs (medication) we usually aim for having a high degree of consistency, so that different batches produce the same product (quality, concentration, etc.). 

Consistency is not trivial to produce: production lines malfunction, shifts change, and machinery is sometimes replaced. 

The random effects model can be used to determine if consistency of the production lines should be rejected.

--

## The model - Olive oil acidity 

   * We are manufacturing olive oil, and each batch should have an acidity level of 0.5%
   
   * The factory wants to design an experiment which will test if the acidity is consistent.
   
   * Questions: What is our factor? What are the factor levels?
   
---

# Example - Olive oil acidity

The batch is our factor. It has many levels and therefore we treat it is a random effect.

--

$$\text{acidity} = \mu + \tau_\text{batch} + \epsilon_{batch,j}$$
.tiny[
```{r olive oil acidity example}

oil_manufacturing <- tibble(
   batch = factor(c("A", "A", "A", "A", "B", "B", "B", "B", "B", 
                    "C", "C", "C", "C", "D", "D", "E", "E")),
   acidity = 
      c(0.45, 0.49, 0.51, 0.50, 0.49, 0.60, 0.55, 0.41, 0.36, 
        0.50, 0.51, 0.49, 0.48, 0.50, 0.51, 0.08, 0.20))

oil_aov <- aov(formula = acidity ~ batch, data = oil_manufacturing)
summary(oil_aov)
TukeyHSD(oil_aov)
```
]

---

# Example - Olive Oil Acidity (cont.)

We can use Dunnett's test to specify the exact contrasts we would like to examine:

.small[
```{r olive oil multiple comparisons}
library(multcomp)
glht(oil_aov,
     linfct = mcp(batch = c("E-A=0", "E-B=0", "E-C=0", "E-D=0"))) %>% 
   summary()
```
]

---

# Randomized Complete Block Design

Sometimes we have an additional factor which is not the aim of the study. For example, imagine a clinical research where $a$ treatments are used in $b$ medical centers.

--

It might be that the fact that there are varying medical centers in itself influences the results.

--

If the selection of treatments is ranodmized across medical centers, the experiment is called a randomized complete block design.

--

|  | Blocks |  |  |  |  |  |
|------------|--------------------|--------------------|---------:|---------------------|------------------|-------------------|
| Treatments | 1 | 2 | $\ldots$ | $b$ | Totals | Averages |
| 1 | $y_{11}$ | $y_{12}$ | $\ldots$ | $y_{1b}$ | $y_{1\cdot}$ | $\bar{y}_{1\cdot}$ |
| $\vdots$ |  |  | $\vdots$ |  |  | $\vdots$ |
| $a$ | $y_{a1}$ | $y_{a2}$ | $\ldots$ | $y_{ab}$ | $y_{a\cdot}$ | $\bar{y}_{a\cdot}$ |
| Totals | $y_{\cdot1}$ | $y_{\cdot2}$ | $\ldots$ | $y_{\cdot b}$ | $y_{\cdot\cdot}$ |  |
| Averages | $\bar{y}_{\cdot1}$ | $\bar{y}_{\cdot2}$ | $\ldots$ | $\bar{y}_{\cdot b}$ |  |  |

---

# Randomized Complete Block Design (cont.)

The formula for this design is given by:

$$Y_{ij}=\mu + \tau_i + \beta_j + \epsilon_{ij}, \quad i\in\{1,\ldots,a\},j\in\{1,\ldots,b\}$$

Where

   * $\mu$ is the overall mean.
   
   * $\tau_i$ is the effect of the $i$th treatment.
   
   * $\beta_j$ is the effect of the $j$th block.
   
   * $\epsilon_{ij}$ is the error term $\epsilon_{ij}\sim\mathcal{N}(0,\sigma)$
   
--

We assume that the treatments and blocks are fixed factors, and that they are deviations from the overall mean, so $\sum_{i=1}^a{\tau_i}=0$ and $\sum_{j=1}^b{\beta_j}=0$. We test the hypothesis:

   * $H_0: \tau_1=\ldots=\tau_a=0$
   * $H_1: \exists i: \tau_i\neq0$

---

# Sum of Squares Identity in the Randomized Complete Block Design

The sum of squares identity can be broken into

$$\sum_{i=1}^a\sum_{j=1}^b(y_{ij}-\bar{y}_{\cdot\cdot})^2 = b\sum_{i=1}^a{(\bar{y}_{i\cdot}-\bar{y}_{\cdot\cdot})^2} + a\sum_{j=1}^b{(\bar{y}_{\cdot j}-\bar{y}_{\cdot\cdot})^2}+\sum_{i,j}{(y_{ij}-\bar{y}_{\cdot j} - \bar{y}_{i\cdot} + \bar{y}_{\cdot\cdot})^2}$$
--

Symbolically stated as

$$SS_T = SS_\text{Treatments} + SS_\text{Blocks} + SS_E$$

--

With degrees of freedom:

$$ab-1 = (a-1) + (b-1) + (a-1)(b-1)$$

---

# Randomized Complete Block Design Hypothesis Test

Set:

$$MS_\text{Treatments} = \frac{SS_\text{Treatments}}{a-1},\quad MS_\text{Blocks} = \frac{SS_\text{Blocks}}{b-1},\quad MS_E=\frac{SS_E}{(a-1)(b-1)}$$

--

The mean sum of squares is given by:

   * $E(MS_\text{Treatments}) = \sigma^2 + \frac{b\sum_{i=1}^a{\tau_i^2}}{a-1}$
   
   * $E(MS_\text{Blocks}) = \sigma^2 + \frac{a\sum_{j=1}^b{\beta_j^2}}{b-1}$
   
   * $E(MS_E) = \sigma^2$
   
--

If $H_0$ is true and all $\tau_i=0$ then:

$$F_0=\frac{MS_\text{Treatments}}{MS_E}$$

Is $F$-distributed with $a-1$ and $(a-1)(b-1)$ degrees of freedom.

---

# Example for Randomized Complete Block Design - ANOVA

An experiment was performed to determine the effect of four chemicals on the fabric strength (Example 13.5 from Montgomery).

.tiny[
```{r montgomery 13.5}
fabric_strength <- read_csv("https://raw.githubusercontent.com/adisarid/intro_statistics_R/master/lectures/data/montgomery_13.5_fabric_strength.csv", col_types = cols()) %>% 
   pivot_longer(cols = -chemical, names_to = "fabric_sample", values_to = "strength") %>% 
   mutate(chemical = factor(chemical))

fabric_aov <- aov(formula = strength ~ chemical + fabric_sample, data = fabric_strength)
summary(fabric_aov)
TukeyHSD(fabric_aov)
```
]